{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingWarmRestarts\n",
    "sys.path.append('../src')\n",
    "from modules import (\n",
    "                    paths,\n",
    "                    dataset,\n",
    "                    model,\n",
    "                    utils,\n",
    "                    acdc,\n",
    "                    train\n",
    "                    )\n",
    "from torchvision.transforms import v2\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = v2.Compose([\n",
    "    v2.Lambda(lambda x: x.convert('RGB')),  # some images are in grayscale\n",
    "    v2.ToImage(), \n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.RandAugment(),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    v2.RandomErasing(p=0.25),\n",
    "\n",
    "])\n",
    "\n",
    "transform_valid = v2.Compose([\n",
    "    v2.Lambda(lambda x: x.convert('RGB')),  # some images are in grayscale\n",
    "    v2.ToImage(), \n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(dataset)\n",
    "if toy == True:\n",
    "    print(\"laoding toy datasets\")\n",
    "    train_dataset, coarse_labels = dataset.load_animal_dataset(\"train\", transform=transform_train, tiny=True, stop=6)\n",
    "    val_dataset, coarse_labels = dataset.load_animal_dataset(\"valid\", transform=transform_valid, tiny=True, stop=2)\n",
    "\n",
    "else:\n",
    "    print(\"loading full dataet\")\n",
    "    train_dataset, coarse_labels = dataset.load_animal_dataset(\"train\", transform=transform_train)\n",
    "    val_dataset, coarse_labels = dataset.load_animal_dataset(\"valid\", transform=transform_valid)\n",
    "\n",
    "train_dataset = dataset.TorchDatasetWrapper(train_dataset, transform=transform_train)\n",
    "val_dataset = dataset.TorchDatasetWrapper(val_dataset, transform=transform_valid)\n",
    "print(\"train:\\n\"+str(train_dataset))\n",
    "print(\"validation:\\n\"+str(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5 if toy else 4096 \n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,  \n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2,  \n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4,\n",
    "    persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(model)\n",
    "config = {\n",
    "    \"patch_size\": 8,           # Kept small for fine-grained patches\n",
    "    \"hidden_size\": 64,          # Increased from 48 (better representation)\n",
    "    \"num_hidden_layers\": 6,     # Deeper for pruning flexibility\n",
    "    \"num_attention_heads\": 8,   # More heads (head_dim = 64/8 = 8)\n",
    "    \"intermediate_size\": 4 * 64,# Standard FFN scaling\n",
    "    \"hidden_dropout_prob\": 0.2, # Mild dropout for regularization\n",
    "    \"attention_probs_dropout_prob\": 0.2,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"image_size\": 64,\n",
    "    \"num_classes\": 58,\n",
    "    \"num_channels\": 3,\n",
    "    \"qkv_bias\": True,           # Keep bias for now (can prune later)\n",
    "}\n",
    "\n",
    "importlib.reload(train)\n",
    "\n",
    "class SoftTargetCrossEntropy(nn.Module):\n",
    "    \"\"\"Cross-entropy loss compatible with Mixup/Cutmix soft labels\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x, target):\n",
    "        # x = model outputs (logits)\n",
    "        # target = mixed labels (probability distributions)\n",
    "        loss = torch.sum(-target * F.log_softmax(x, dim=1), dim=1)\n",
    "        return loss.mean()\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "vit = model.ViT(config).to(device)\n",
    "\n",
    "num_epochs = 500\n",
    "warmup_epochs = 20\n",
    "base_lr = 3e-4\n",
    "min_lr = 1e-6\n",
    "weight_decay = 0.05  # For AdamW optimizer\n",
    "label_smoothing = 0.1  # For cross-entropy\n",
    "patience = 50\n",
    "\n",
    "\n",
    "\n",
    "optimizer = AdamW(vit.parameters(),\n",
    "                  lr=base_lr,\n",
    "                  weight_decay = weight_decay,\n",
    "                  betas=(0.9, 0.98),\n",
    "                  eps = 1e-6      \n",
    "                  )\n",
    "\n",
    "# Linear warmup for 30 epochs (0 â†’ base_lr)\n",
    "warmup = LinearLR(\n",
    "    optimizer,\n",
    "    start_factor=1e-6,  # Near-zero initial LR\n",
    "    end_factor=1.0,     # Full LR after warmup\n",
    "    total_iters=warmup_epochs,\n",
    ")\n",
    "\n",
    "cosine = CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=num_epochs - warmup_epochs,  \n",
    "    eta_min=min_lr,\n",
    ")\n",
    "\n",
    "# Combine them\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[warmup, cosine],\n",
    "    milestones=[warmup_epochs], \n",
    ")\n",
    "\n",
    "mixup_fn = v2.MixUp(\n",
    "    alpha=1.0,          \n",
    "    num_classes=58\n",
    ")\n",
    "\n",
    "trainer = train.Trainer(model=vit,\n",
    "                        train_loader=train_loader,\n",
    "                        val_loader=val_loader,\n",
    "                        optimizer=optimizer,\n",
    "                        criterion=SoftTargetCrossEntropy(),\n",
    "                        val_criterion=nn.CrossEntropyLoss(),\n",
    "                        scheduler=scheduler,\n",
    "                        device = device,\n",
    "                        writer=torch.utils.tensorboard.SummaryWriter(log_dir=paths.logs),\n",
    "                        scaler=torch.amp.GradScaler(),\n",
    "                        num_epochs=num_epochs,\n",
    "                        log_interval=50,\n",
    "                        model_dir=paths.chekpoints,\n",
    "                        mixup_fn=mixup_fn,\n",
    "                        early_stop_patience=20,\n",
    "                        model_name=\"vit1\",\n",
    "                        resume=True\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(acdc)\n",
    "importlib.reload(dataset)\n",
    "acdc_dataset = dataset.ContrastiveWrapper(val_dataset, coarse_labels)\n",
    "\n",
    "acdc_loader = DataLoader(\n",
    "    acdc_dataset,\n",
    "    batch_size=50,\n",
    "    shuffle=False,\n",
    "    # num_workers=1,  \n",
    "    # pin_memory=False,\n",
    "    # prefetch_factor=1,\n",
    "    collate_fn=dataset.contrastive_collate_fn,\n",
    "    # persistent_workers=False\n",
    ")\n",
    "clean_batch, corrupted_batch = next(iter(acdc_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(acdc)\n",
    "importlib.reload(utils)\n",
    "run_acdc = False\n",
    "if run_acdc:\n",
    "    circuits = {}\n",
    "    for tau in [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "        circuits[str(tau)] = acdc.run_ACDC_optimized(vit, tau, acdc_loader, device=device)\n",
    "else:\n",
    "    cirtcuits_paths = paths.chekpoints / \"circuits0001.pkl\"\n",
    "    import pickle\n",
    "    circuits = pickle.load(open(cirtcuits_paths, \"rb\"))\n",
    "    cirtcuits_paths = paths.chekpoints / \"circuits.pkl\"\n",
    "    circuits = circuits | pickle.load(open(cirtcuits_paths, \"rb\"))\n",
    "    print(circuits.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuits[\"0.001\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == torch.device(\"cpu\"): val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "acdc.test_taus(circuits, val_loader, coarse_labels, config, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def tau_formatter(x, pos):\n",
    "    if x < 0.0005:\n",
    "        return f'{x:.4f}'\n",
    "    elif x < 0.005:\n",
    "        return f'{x:.4f}'\n",
    "    else:\n",
    "        return f'{x:.3f}'\n",
    "\n",
    "def parse_results(text):\n",
    "    taus = []\n",
    "    accuracies = []\n",
    "    times = []\n",
    "    \n",
    "    # Extract data using regular expressions\n",
    "    tau_matches = re.findall(r'Testing model with tau ([\\d.]+)', text)\n",
    "    accuracy_matches = re.findall(r'Accuracy: ([\\d.]+)', text)\n",
    "    time_matches = re.findall(r'Avg Time over 10 runs: ([\\d.]+) seconds', text)\n",
    "    \n",
    "    # Convert extracted strings to numbers\n",
    "    taus = [float(tau) for tau in tau_matches]\n",
    "    accuracies = [float(acc) for acc in accuracy_matches]\n",
    "    times = [float(t) for t in time_matches]\n",
    "    \n",
    "    # Find baseline time (tau=0.0001)\n",
    "    baseline_index = taus.index(0.0001)\n",
    "    baseline_time = times[baseline_index]\n",
    "    \n",
    "    # Calculate speedup ratios\n",
    "    speedups = [baseline_time / t for t in times]\n",
    "    \n",
    "    # Sort all data by tau values\n",
    "    sorted_indices = np.argsort(taus)\n",
    "    taus_sorted = np.array(taus)[sorted_indices]\n",
    "    accuracies_sorted = np.array(accuracies)[sorted_indices]\n",
    "    speedups_sorted = np.array(speedups)[sorted_indices]\n",
    "    \n",
    "    return taus_sorted, accuracies_sorted, speedups_sorted\n",
    "\n",
    "def plot_graphs(taus, accuracies, speedups):\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    \n",
    "    # Custom formatter for tau values\n",
    "    def tau_formatter(x, pos):\n",
    "        if x < 0.0005:\n",
    "            return f'{x:.4f}'\n",
    "        elif x < 0.005:\n",
    "            return f'{x:.4f}'\n",
    "        else:\n",
    "            return f'{x:.3f}'\n",
    "    \n",
    "    # Accuracy vs Tau plot\n",
    "    ax1 = plt.subplot(2, 1, 1)\n",
    "    plt.semilogx(taus, accuracies, 'bo-', linewidth=2, markersize=8)\n",
    "    plt.title('Model Accuracy vs. Pruning Threshold (Ï„)', fontsize=14, pad=20)\n",
    "    plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "    plt.grid(True, which='both', linestyle='--', alpha=0.7)\n",
    "    plt.axvline(x=0.001, color='r', linestyle='--', alpha=0.7, \n",
    "                label='Optimal Ï„ (0.001)')\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 70)\n",
    "    \n",
    "    # Add tau values below points\n",
    "    for tau, acc in zip(taus, accuracies):\n",
    "        plt.annotate(tau_formatter(tau, None), \n",
    "                    xy=(tau, 0), \n",
    "                    xytext=(0, -40), \n",
    "                    textcoords='offset points',\n",
    "                    ha='center', va='top', \n",
    "                    rotation=45, fontsize=9,\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.1\", fc=\"white\", ec=\"gray\", alpha=0.7))\n",
    "    \n",
    "    # Speedup vs Tau plot\n",
    "    ax2 = plt.subplot(2, 1, 2)\n",
    "    plt.semilogx(taus, speedups, 'go-', linewidth=2, markersize=8)\n",
    "    plt.title('Inference Speedup vs. Pruning Threshold (Ï„)', fontsize=14, pad=20)\n",
    "    plt.xlabel('Ï„ (log scale)', fontsize=12)\n",
    "    plt.ylabel('Speedup Ratio (baseline = Ï„=0.0001)', fontsize=12)\n",
    "    plt.grid(True, which='both', linestyle='--', alpha=0.7)\n",
    "    plt.axhline(y=1, color='k', linestyle='-', alpha=0.5)\n",
    "    \n",
    "    # Add tau values below points\n",
    "    min_speedup = min(speedups)\n",
    "    for tau, spd in zip(taus, speedups):\n",
    "        plt.annotate(tau_formatter(tau, None), \n",
    "                    xy=(tau, min_speedup), \n",
    "                    xytext=(0, -40), \n",
    "                    textcoords='offset points',\n",
    "                    ha='center', va='top', \n",
    "                    rotation=45, fontsize=9,\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.1\", fc=\"white\", ec=\"gray\", alpha=0.7))\n",
    "    \n",
    "    # Adjust layout to make room for labels\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.15, hspace=0.3)\n",
    "    \n",
    "    plt.savefig('pruning_analysis_with_tau_labels.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "with open(paths.data/\"benchmark ryzen74700u.txt\", 'r') as f:\n",
    "    data = f.read()\n",
    "    \n",
    "    taus, accuracies, speedups = parse_results(data)\n",
    "    plot_graphs(taus, accuracies, speedups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(model)\n",
    "config = {\n",
    "    \"patch_size\": 8,           # Kept small for fine-grained patches\n",
    "    \"hidden_size\": 64,          # Increased from 48 (better representation)\n",
    "    \"num_hidden_layers\": 6,     # Deeper for pruning flexibility\n",
    "    \"num_attention_heads\": 8,   # More heads (head_dim = 64/8 = 8)\n",
    "    \"intermediate_size\": 4 * 64,# Standard FFN scaling\n",
    "    \"hidden_dropout_prob\": 0.2, # Mild dropout for regularization\n",
    "    \"attention_probs_dropout_prob\": 0.2,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"image_size\": 64,\n",
    "    \"num_classes\": 58,\n",
    "    \"num_channels\": 3,\n",
    "    \"qkv_bias\": True,           # Keep bias for now (can prune later)\n",
    "}\n",
    "vit = model.ViT(config)\n",
    "#Accuracy: 56.10344827586207 | Avg Time over 10 runs: 5.18979434967041 seconds\n",
    "vit.retain_circuit(circuits[\"0.004\"])\n",
    "vit = vit.to(device)\n",
    "\n",
    "class SoftTargetCrossEntropy(nn.Module):\n",
    "    \"\"\"Cross-entropy loss compatible with Mixup/Cutmix soft labels\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x, target):\n",
    "        # x = model outputs (logits)\n",
    "        # target = mixed labels (probability distributions)\n",
    "        loss = torch.sum(-target * F.log_softmax(x, dim=1), dim=1)\n",
    "        return loss.mean()\n",
    "\n",
    "num_epochs = 500\n",
    "warmup_epochs = 20\n",
    "base_lr = 3e-4\n",
    "min_lr = 1e-6\n",
    "weight_decay = 0.05  # For AdamW optimizer\n",
    "label_smoothing = 0.1  # For cross-entropy\n",
    "patience = 50\n",
    "\n",
    "optimizer = AdamW(vit.parameters(),\n",
    "                  lr=base_lr,\n",
    "                  weight_decay = weight_decay,\n",
    "                  betas=(0.9, 0.98),\n",
    "                  eps = 1e-6      \n",
    "                  )\n",
    "\n",
    "# Linear warmup for 30 epochs (0 â†’ base_lr)\n",
    "warmup = LinearLR(\n",
    "    optimizer,\n",
    "    start_factor=1e-6,  # Near-zero initial LR\n",
    "    end_factor=1.0,     # Full LR after warmup\n",
    "    total_iters=warmup_epochs,\n",
    ")\n",
    "\n",
    "cosine = CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=num_epochs - warmup_epochs,  \n",
    "    eta_min=min_lr,\n",
    ")\n",
    "\n",
    "# Combine them\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[warmup, cosine],\n",
    "    milestones=[warmup_epochs], \n",
    ")\n",
    "\n",
    "mixup_fn = v2.MixUp(\n",
    "    alpha=1.0,          \n",
    "    num_classes=58\n",
    ")\n",
    "\n",
    "trainer = train.Trainer(model=vit,\n",
    "                        train_loader=train_loader,\n",
    "                        val_loader=val_loader,\n",
    "                        optimizer=optimizer,\n",
    "                        criterion=SoftTargetCrossEntropy(),\n",
    "                        val_criterion=nn.CrossEntropyLoss(),\n",
    "                        scheduler=scheduler,\n",
    "                        device = device,\n",
    "                        writer=torch.utils.tensorboard.SummaryWriter(log_dir=paths.logs),\n",
    "                        scaler=torch.amp.GradScaler(),\n",
    "                        num_epochs=num_epochs,\n",
    "                        log_interval=50,\n",
    "                        model_dir=paths.chekpoints,\n",
    "                        mixup_fn=mixup_fn,\n",
    "                        early_stop_patience=20,\n",
    "                        model_name=\"pruned_vit_fine.pth\",\n",
    "                        resume=False\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
