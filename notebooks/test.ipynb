{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingWarmRestarts\n",
    "sys.path.append('../src')\n",
    "from modules import (\n",
    "                    paths,\n",
    "                    dataset,\n",
    "                    model,\n",
    "                    utils,\n",
    "                    acdc,\n",
    "                    train\n",
    "                    )\n",
    "from torchvision.transforms import v2\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = v2.Compose([\n",
    "    v2.Lambda(lambda x: x.convert('RGB')),  # some images are in grayscale\n",
    "    v2.ToImage(), \n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.RandAugment(),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    v2.RandomErasing(p=0.25),\n",
    "\n",
    "])\n",
    "\n",
    "transform_valid = v2.Compose([\n",
    "    v2.Lambda(lambda x: x.convert('RGB')),  # some images are in grayscale\n",
    "    v2.ToImage(), \n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(dataset)\n",
    "if toy == True:\n",
    "    print(\"laoding toy datasets\")\n",
    "    train_dataset, coarse_labels = dataset.load_animal_dataset(\"train\", transform=transform_train, tiny=True, stop=6)\n",
    "    val_dataset, coarse_labels = dataset.load_animal_dataset(\"valid\", transform=transform_valid, tiny=True, stop=2)\n",
    "\n",
    "else:\n",
    "    print(\"loading full dataet\")\n",
    "    train_dataset, coarse_labels = dataset.load_animal_dataset(\"train\", transform=transform_train)\n",
    "    val_dataset, coarse_labels = dataset.load_animal_dataset(\"valid\", transform=transform_valid)\n",
    "\n",
    "train_dataset = dataset.TorchDatasetWrapper(train_dataset, transform=transform_train)\n",
    "val_dataset = dataset.TorchDatasetWrapper(val_dataset, transform=transform_valid)\n",
    "print(\"train:\\n\"+str(train_dataset))\n",
    "print(\"validation:\\n\"+str(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5 if toy else 4096 \n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,  \n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2,  \n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4,\n",
    "    persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(model)\n",
    "config = {\n",
    "    \"patch_size\": 8,           # Kept small for fine-grained patches\n",
    "    \"hidden_size\": 64,          # Increased from 48 (better representation)\n",
    "    \"num_hidden_layers\": 6,     # Deeper for pruning flexibility\n",
    "    \"num_attention_heads\": 8,   # More heads (head_dim = 64/8 = 8)\n",
    "    \"intermediate_size\": 4 * 64,# Standard FFN scaling\n",
    "    \"hidden_dropout_prob\": 0.2, # Mild dropout for regularization\n",
    "    \"attention_probs_dropout_prob\": 0.2,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"image_size\": 64,\n",
    "    \"num_classes\": 58,\n",
    "    \"num_channels\": 3,\n",
    "    \"qkv_bias\": True,           # Keep bias for now (can prune later)\n",
    "}\n",
    "\n",
    "importlib.reload(train)\n",
    "\n",
    "class SoftTargetCrossEntropy(nn.Module):\n",
    "    \"\"\"Cross-entropy loss compatible with Mixup/Cutmix soft labels\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x, target):\n",
    "        # x = model outputs (logits)\n",
    "        # target = mixed labels (probability distributions)\n",
    "        loss = torch.sum(-target * F.log_softmax(x, dim=1), dim=1)\n",
    "        return loss.mean()\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "vit = model.ViT(config).to(device)\n",
    "\n",
    "num_epochs = 500\n",
    "warmup_epochs = 20\n",
    "base_lr = 3e-4\n",
    "min_lr = 1e-6\n",
    "weight_decay = 0.05  # For AdamW optimizer\n",
    "label_smoothing = 0.1  # For cross-entropy\n",
    "patience = 50\n",
    "\n",
    "\n",
    "\n",
    "optimizer = AdamW(vit.parameters(),\n",
    "                  lr=base_lr,\n",
    "                  weight_decay = weight_decay,\n",
    "                  betas=(0.9, 0.98),\n",
    "                  eps = 1e-6      \n",
    "                  )\n",
    "\n",
    "# Linear warmup for 30 epochs (0 â†’ base_lr)\n",
    "warmup = LinearLR(\n",
    "    optimizer,\n",
    "    start_factor=1e-6,  # Near-zero initial LR\n",
    "    end_factor=1.0,     # Full LR after warmup\n",
    "    total_iters=warmup_epochs,\n",
    ")\n",
    "\n",
    "cosine = CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=num_epochs - warmup_epochs,  \n",
    "    eta_min=min_lr,\n",
    ")\n",
    "\n",
    "# Combine them\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[warmup, cosine],\n",
    "    milestones=[warmup_epochs], \n",
    ")\n",
    "\n",
    "mixup_fn = v2.MixUp(\n",
    "    alpha=1.0,          \n",
    "    num_classes=58\n",
    ")\n",
    "\n",
    "trainer = train.Trainer(model=vit,\n",
    "                        train_loader=train_loader,\n",
    "                        val_loader=val_loader,\n",
    "                        optimizer=optimizer,\n",
    "                        criterion=SoftTargetCrossEntropy(),\n",
    "                        val_criterion=nn.CrossEntropyLoss(),\n",
    "                        scheduler=scheduler,\n",
    "                        device = device,\n",
    "                        writer=torch.utils.tensorboard.SummaryWriter(log_dir=paths.logs),\n",
    "                        scaler=torch.amp.GradScaler(),\n",
    "                        num_epochs=num_epochs,\n",
    "                        log_interval=50,\n",
    "                        model_dir=paths.chekpoints,\n",
    "                        mixup_fn=mixup_fn,\n",
    "                        early_stop_patience=20,\n",
    "                        model_name=\"vit1\",\n",
    "                        resume=True\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(acdc)\n",
    "importlib.reload(dataset)\n",
    "acdc_dataset = dataset.ContrastiveWrapper(val_dataset, coarse_labels)\n",
    "\n",
    "acdc_loader = DataLoader(\n",
    "    acdc_dataset,\n",
    "    batch_size=50,\n",
    "    shuffle=False,\n",
    "    # num_workers=1,  \n",
    "    # pin_memory=False,\n",
    "    # prefetch_factor=1,\n",
    "    collate_fn=dataset.contrastive_collate_fn,\n",
    "    # persistent_workers=False\n",
    ")\n",
    "clean_batch, corrupted_batch = next(iter(acdc_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(acdc)\n",
    "importlib.reload(utils)\n",
    "run_acdc = False\n",
    "if run_acdc:\n",
    "    circuits = {}\n",
    "    for tau in [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "        circuits[str(tau)] = acdc.run_ACDC(vit, tau, acdc_loader, device=device)\n",
    "else:\n",
    "    cirtcuits_paths = paths.chekpoints / \"circuits.pkl\"\n",
    "    import pickle\n",
    "    circuits = pickle.load(open(cirtcuits_paths, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def get_accuracy_on_coarse_labels(model, datalaoder, device, coarse_model=False) -> float:\n",
    "    fine_label_to_coarse = {fl:cli for cli, cl in enumerate(coarse_labels.values()) for fl in cl}\n",
    "    coarse_to_name = {i:cl for i, cl in enumerate(coarse_labels.keys())}\n",
    "\n",
    "    model.eval()\n",
    "    correct, total = 0.0, 0\n",
    "\n",
    "    dataloader_tqdm = tqdm(\n",
    "        datalaoder, \n",
    "        desc=f\"[Validation]\", \n",
    "        leave=False\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels) in enumerate(dataloader_tqdm):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            labels.apply_(lambda x: fine_label_to_coarse.get(x, x))\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type):\n",
    "                outputs, _ = model(images)\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            if not coarse_model:\n",
    "                predicted.apply_(lambda x: fine_label_to_coarse.get(x, x))\n",
    "            batch_correct = predicted.eq(labels).sum().item()\n",
    "            batch_total = labels.size(0)\n",
    "\n",
    "            correct += batch_correct\n",
    "            total += batch_total\n",
    "            \n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    return epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(model)\n",
    "\n",
    "trainer.load_checkpoint(paths.chekpoints/\"checkpoint.pth\")\n",
    "vit = trainer.model\n",
    "import time\n",
    "start = time.time()\n",
    "print(get_accuracy_on_coarse_labels(vit, val_loader, device, coarse_model=False))\n",
    "end = time.time()\n",
    "print(f\"took: {end-start}\")\n",
    "vit.retrain_circuit(circuits[\"0.001\"])\n",
    "start = time.time()\n",
    "print(get_accuracy_on_coarse_labels(vit, val_loader, device, coarse_model=False))\n",
    "end = time.time()\n",
    "print(f\"took: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(model)\n",
    "vit.classifier = nn.Linear(config[\"hidden_size\"], len(coarse_labels))\n",
    "vit.apply(vit._init_weights)\n",
    "vit = vit.to(device)\n",
    "\n",
    "class SoftTargetCrossEntropy(nn.Module):\n",
    "    \"\"\"Cross-entropy loss compatible with Mixup/Cutmix soft labels\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x, target):\n",
    "        # x = model outputs (logits)\n",
    "        # target = mixed labels (probability distributions)\n",
    "        loss = torch.sum(-target * F.log_softmax(x, dim=1), dim=1)\n",
    "        return loss.mean()\n",
    "    \n",
    "num_epochs = 500\n",
    "warmup_epochs = 20\n",
    "base_lr = 3e-4\n",
    "min_lr = 1e-6\n",
    "weight_decay = 0.05  \n",
    "label_smoothing = 0.1  \n",
    "\n",
    "\n",
    "\n",
    "optimizer = AdamW(vit.parameters(),\n",
    "                  lr=base_lr,\n",
    "                  weight_decay = weight_decay,\n",
    "                  betas=(0.9, 0.98),\n",
    "                  eps = 1e-6      \n",
    "                  )\n",
    "\n",
    "warmup = LinearLR(\n",
    "    optimizer,\n",
    "    start_factor=1e-6,  # Near-zero initial LR\n",
    "    end_factor=1.0,     # Full LR after warmup\n",
    "    total_iters=warmup_epochs,\n",
    ")\n",
    "\n",
    "cosine = CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=num_epochs - warmup_epochs,  \n",
    "    eta_min=min_lr,\n",
    ")\n",
    "\n",
    "# Combine them\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[warmup, cosine],\n",
    "    milestones=[warmup_epochs],  \n",
    ")\n",
    "\n",
    "mixup_fn = v2.MixUp(\n",
    "    alpha=1.0,          \n",
    "    num_classes=58\n",
    ")\n",
    "\n",
    "trainer = train.CoarseTrainer(coarse_labels,\n",
    "                            model=vit,\n",
    "                            train_loader=train_loader,\n",
    "                            val_loader=val_loader,\n",
    "                            optimizer=optimizer,\n",
    "                            criterion=SoftTargetCrossEntropy(),\n",
    "                            val_criterion=nn.CrossEntropyLoss(),\n",
    "                            scheduler=scheduler,\n",
    "                            device = device,\n",
    "                            writer=torch.utils.tensorboard.SummaryWriter(log_dir=paths.logs),\n",
    "                            scaler=torch.amp.GradScaler(),\n",
    "                            num_epochs=num_epochs,\n",
    "                            log_interval=50,\n",
    "                            model_dir=paths.chekpoints,\n",
    "                            mixup_fn=mixup_fn,\n",
    "                            early_stop_patience=20,\n",
    "                            model_name=\"vit1\",\n",
    "                            resume=False\n",
    "                            )\n",
    "acc = trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
