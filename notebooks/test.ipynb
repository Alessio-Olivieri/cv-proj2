{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lexyo/Dev/cv-proj2/notebooks/../src/modules/paths.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexyo/Dev/cv-proj2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/lexyo/.config/matplotlib is not a writable directory\n",
      "Matplotlib created a temporary cache directory at /tmp/matplotlib-5ke2ctk7 because there was an issue with the default path (/home/lexyo/.config/matplotlib); it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingWarmRestarts\n",
    "sys.path.append('../src')\n",
    "from modules import (\n",
    "                    paths,\n",
    "                    dataset,\n",
    "                    model,\n",
    "                    utils,\n",
    "                    acdc,\n",
    "                    train\n",
    "                    )\n",
    "from torchvision.transforms import v2\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = v2.Compose([\n",
    "    v2.Lambda(lambda x: x.convert('RGB')),  # some images are in grayscale\n",
    "    v2.ToImage(), \n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.RandAugment(),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    v2.RandomErasing(p=0.25),\n",
    "\n",
    "])\n",
    "\n",
    "transform_valid = v2.Compose([\n",
    "    v2.Lambda(lambda x: x.convert('RGB')),  # some images are in grayscale\n",
    "    v2.ToImage(), \n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading full dataet\n",
      "Generating animal dataset...\n",
      "Loading dataset from /home/lexyo/Dev/cv-proj2/notebooks/../data/train.pkl\n",
      "Generating animal dataset...\n",
      "Loading dataset from /home/lexyo/Dev/cv-proj2/notebooks/../data/valid.pkl\n",
      "train:\n",
      "Dataset({\n",
      "    features: ['image', 'label'],\n",
      "    num_rows: 29000\n",
      "})\n",
      "validation:\n",
      "Dataset({\n",
      "    features: ['image', 'label'],\n",
      "    num_rows: 2900\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(dataset)\n",
    "if toy == True:\n",
    "    print(\"laoding toy datasets\")\n",
    "    train_dataset, coarse_labels = dataset.load_animal_dataset(\"train\", transform=transform_train, tiny=True, stop=6)\n",
    "    val_dataset, coarse_labels = dataset.load_animal_dataset(\"valid\", transform=transform_valid, tiny=True, stop=2)\n",
    "\n",
    "else:\n",
    "    print(\"loading full dataet\")\n",
    "    train_dataset, coarse_labels = dataset.load_animal_dataset(\"train\", transform=transform_train)\n",
    "    val_dataset, coarse_labels = dataset.load_animal_dataset(\"valid\", transform=transform_valid)\n",
    "\n",
    "train_dataset = dataset.TorchDatasetWrapper(train_dataset, transform=transform_train)\n",
    "val_dataset = dataset.TorchDatasetWrapper(val_dataset, transform=transform_valid)\n",
    "print(\"train:\\n\"+str(train_dataset))\n",
    "print(\"validation:\\n\"+str(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5 if toy else 4096 \n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,  \n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2,  \n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4,\n",
    "    persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from checkpoint: /home/lexyo/Dev/cv-proj2/notebooks/../checkpoints/checkpoint.pth\n",
      "Checkpoint loaded. Resuming from epoch 387 with best accuracy 21.55%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexyo/Dev/cv-proj2/.venv/lib/python3.12/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(model)\n",
    "config = {\n",
    "    \"patch_size\": 8,           # Kept small for fine-grained patches\n",
    "    \"hidden_size\": 64,          # Increased from 48 (better representation)\n",
    "    \"num_hidden_layers\": 6,     # Deeper for pruning flexibility\n",
    "    \"num_attention_heads\": 8,   # More heads (head_dim = 64/8 = 8)\n",
    "    \"intermediate_size\": 4 * 64,# Standard FFN scaling\n",
    "    \"hidden_dropout_prob\": 0.2, # Mild dropout for regularization\n",
    "    \"attention_probs_dropout_prob\": 0.2,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"image_size\": 64,\n",
    "    \"num_classes\": 57,\n",
    "    \"num_channels\": 3,\n",
    "    \"qkv_bias\": True,           # Keep bias for now (can prune later)\n",
    "}\n",
    "\n",
    "importlib.reload(train)\n",
    "\n",
    "class SoftTargetCrossEntropy(nn.Module):\n",
    "    \"\"\"Cross-entropy loss compatible with Mixup/Cutmix soft labels\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x, target):\n",
    "        # x = model outputs (logits)\n",
    "        # target = mixed labels (probability distributions)\n",
    "        loss = torch.sum(-target * F.log_softmax(x, dim=1), dim=1)\n",
    "        return loss.mean()\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "vit = model.ViT(config).to(device)\n",
    "\n",
    "num_epochs = 500\n",
    "warmup_epochs = 20\n",
    "base_lr = 3e-4\n",
    "min_lr = 1e-6\n",
    "weight_decay = 0.05  # For AdamW optimizer\n",
    "label_smoothing = 0.1  # For cross-entropy\n",
    "patience = 50\n",
    "\n",
    "\n",
    "\n",
    "optimizer = AdamW(vit.parameters(),\n",
    "                  lr=base_lr,\n",
    "                  weight_decay = weight_decay,\n",
    "                  betas=(0.9, 0.98),\n",
    "                  eps = 1e-6      \n",
    "                  )\n",
    "\n",
    "# Linear warmup for 30 epochs (0 â†’ base_lr)\n",
    "warmup = LinearLR(\n",
    "    optimizer,\n",
    "    start_factor=1e-6,  # Near-zero initial LR\n",
    "    end_factor=1.0,     # Full LR after warmup\n",
    "    total_iters=warmup_epochs,\n",
    ")\n",
    "\n",
    "# Cosine decay for remaining epochs (170)\n",
    "cosine = CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=num_epochs - warmup_epochs,  # 170 epochs per cycle\n",
    "    eta_min=min_lr,\n",
    ")\n",
    "\n",
    "# Combine them\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[warmup, cosine],\n",
    "    milestones=[warmup_epochs],  # Switch after warmup\n",
    ")\n",
    "\n",
    "mixup_fn = v2.MixUp(\n",
    "    alpha=1.0,          # Add CutMix\n",
    "    num_classes=57\n",
    ")\n",
    "\n",
    "trainer = train.Trainer(model=vit,\n",
    "                        train_loader=train_loader,\n",
    "                        val_loader=val_loader,\n",
    "                        optimizer=optimizer,\n",
    "                        criterion=SoftTargetCrossEntropy(),\n",
    "                        val_criterion=nn.CrossEntropyLoss(),\n",
    "                        scheduler=scheduler,\n",
    "                        device = device,\n",
    "                        writer=torch.utils.tensorboard.SummaryWriter(log_dir=paths.logs),\n",
    "                        scaler=torch.amp.GradScaler(),\n",
    "                        num_epochs=num_epochs,\n",
    "                        log_interval=50,\n",
    "                        model_dir=paths.chekpoints,\n",
    "                        mixup_fn=mixup_fn,\n",
    "                        early_stop_patience=20,\n",
    "                        model_name=\"vit1\",\n",
    "                        resume=True\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing dataset by class for contrastive sampling...\n",
      "Indexing complete.\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(acdc)\n",
    "importlib.reload(dataset)\n",
    "acdc_dataset = dataset.ContrastiveWrapper(val_dataset, coarse_labels)\n",
    "\n",
    "acdc_loader = DataLoader(\n",
    "    acdc_dataset,\n",
    "    batch_size=50,\n",
    "    shuffle=False,\n",
    "    # num_workers=1,  \n",
    "    # pin_memory=False,\n",
    "    # prefetch_factor=1,\n",
    "    collate_fn=dataset.contrastive_collate_fn,\n",
    "    # persistent_workers=False\n",
    ")\n",
    "clean_batch, corrupted_batch = next(iter(acdc_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded. Resuming from epoch 387 with best accuracy 21.55%.\n"
     ]
    }
   ],
   "source": [
    "trainer.load_checkpoint(paths.chekpoints/\"checkpoint.pth\")\n",
    "vit = trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(acdc)\n",
    "importlib.reload(utils)\n",
    "\n",
    "# circuits = {}\n",
    "# for tau in [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "#     circuits[str(tau)] = acdc.run_ACDC(vit, tau, acdc_loader, device=device)\n",
    "circuits = {'0.05': {('embedding.final_output', 'encoder.blocks.0.mlp.final_output'),\n",
    "  ('encoder.blocks.0.mlp.final_output', 'encoder.blocks.1.mlp.final_output'),\n",
    "  ('encoder.blocks.0.mlp.final_output', 'encoder.blocks.2.mlp.final_output'),\n",
    "  ('encoder.blocks.0.mlp.final_output', 'encoder.blocks.3.mlp.final_output')},\n",
    " '0.1': {('embedding.final_output', 'encoder.blocks.0.mlp.final_output'),\n",
    "  ('encoder.blocks.0.mlp.final_output', 'encoder.blocks.1.mlp.final_output'),\n",
    "  ('encoder.blocks.0.mlp.final_output', 'encoder.blocks.2.mlp.final_output')},\n",
    " '0.2': {('embedding.final_output', 'encoder.blocks.0.mlp.final_output'),\n",
    "  ('encoder.blocks.0.mlp.final_output', 'encoder.blocks.1.mlp.final_output')},\n",
    " '0.3': set(),\n",
    " '0.4': set(),\n",
    " '0.5': set(),\n",
    " '0.6': set(),\n",
    " '0.7': set(),\n",
    " '0.8': set(),\n",
    " '0.9': set()}\n",
    "circuits['0.01'] = {('embedding.final_output', 'encoder.blocks.0.attention.heads.4.final_output'),\n",
    " ('embedding.final_output', 'encoder.blocks.0.mlp.final_output'),\n",
    " ('embedding.final_output', 'encoder.blocks.1.mlp.final_output'),\n",
    " ('encoder.blocks.0.attention.heads.4.final_output',\n",
    "  'encoder.blocks.0.mlp.final_output'),\n",
    " ('encoder.blocks.0.mlp.final_output',\n",
    "  'encoder.blocks.1.attention.heads.2.final_output'),\n",
    " ('encoder.blocks.0.mlp.final_output',\n",
    "  'encoder.blocks.1.attention.heads.4.final_output'),\n",
    " ('encoder.blocks.0.mlp.final_output', 'encoder.blocks.1.mlp.final_output'),\n",
    " ('encoder.blocks.0.mlp.final_output', 'encoder.blocks.2.mlp.final_output'),\n",
    " ('encoder.blocks.0.mlp.final_output', 'encoder.blocks.3.mlp.final_output'),\n",
    " ('encoder.blocks.0.mlp.final_output', 'encoder.blocks.4.mlp.final_output'),\n",
    " ('encoder.blocks.0.mlp.final_output', 'encoder.blocks.5.mlp.final_output'),\n",
    " ('encoder.blocks.1.attention.heads.2.final_output',\n",
    "  'encoder.blocks.1.mlp.final_output'),\n",
    " ('encoder.blocks.1.attention.heads.3.final_output',\n",
    "  'encoder.blocks.1.mlp.final_output'),\n",
    " ('encoder.blocks.1.attention.heads.4.final_output',\n",
    "  'encoder.blocks.1.mlp.final_output'),\n",
    " ('encoder.blocks.1.attention.heads.4.final_output',\n",
    "  'encoder.blocks.2.mlp.final_output'),\n",
    " ('encoder.blocks.1.attention.heads.6.final_output',\n",
    "  'encoder.blocks.1.mlp.final_output'),\n",
    " ('encoder.blocks.1.attention.heads.7.final_output',\n",
    "  'encoder.blocks.1.mlp.final_output'),\n",
    " ('encoder.blocks.1.mlp.final_output', 'encoder.blocks.2.mlp.final_output'),\n",
    " ('encoder.blocks.1.mlp.final_output', 'encoder.blocks.3.mlp.final_output'),\n",
    " ('encoder.blocks.1.mlp.final_output', 'encoder.blocks.4.mlp.final_output'),\n",
    " ('encoder.blocks.1.mlp.final_output', 'encoder.blocks.5.mlp.final_output'),\n",
    " ('encoder.blocks.2.mlp.final_output', 'encoder.blocks.3.mlp.final_output'),\n",
    " ('encoder.blocks.2.mlp.final_output', 'encoder.blocks.4.mlp.final_output'),\n",
    " ('encoder.blocks.2.mlp.final_output', 'encoder.blocks.5.mlp.final_output'),\n",
    " ('encoder.blocks.3.mlp.final_output', 'encoder.blocks.4.mlp.final_output'),\n",
    " ('encoder.blocks.3.mlp.final_output', 'encoder.blocks.5.mlp.final_output'),\n",
    " ('encoder.blocks.4.mlp.final_output', 'encoder.blocks.5.mlp.final_output')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'in <string>' requires string as left operand, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m coarse_labels_int = {i:cl \u001b[38;5;28;01mfor\u001b[39;00m i, cl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(coarse_labels)}\n\u001b[32m      3\u001b[39m t = torch.tensor([\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m coarse_labels_batch = [cl \u001b[38;5;28;01mfor\u001b[39;00m cl, fl \u001b[38;5;129;01min\u001b[39;00m coarse_labels_int.items() \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m t \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfl\u001b[49m]\n",
      "\u001b[31mTypeError\u001b[39m: 'in <string>' requires string as left operand, not Tensor"
     ]
    }
   ],
   "source": [
    "coarse_labels\n",
    "coarse_labels_int = {i:cl for i, cl in enumerate(coarse_labels)}\n",
    "t = torch.tensor([0, 0, 1, 1, 2])\n",
    "coarse_labels_batch = [cl for cl, fl in coarse_labels_int.items() for label in t if label in fl]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Aquatic',\n",
       " 1: 'Amphibians & Reptiles',\n",
       " 2: 'Arthropods',\n",
       " 3: 'Birds',\n",
       " 4: 'Mammals',\n",
       " 5: 'Marine Life & Fossils'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coarse_labels_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'val_loss' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoarse_labels\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mget_accuracy\u001b[39m\u001b[34m(model, datalaoder, coarse_labels)\u001b[39m\n\u001b[32m     24\u001b[39m batch_correct = predicted.eq(labels).sum().item()\n\u001b[32m     25\u001b[39m batch_total = labels.size(\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mval_loss\u001b[49m += batch_loss * batch_total\n\u001b[32m     28\u001b[39m correct += batch_correct\n\u001b[32m     29\u001b[39m total += batch_total\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'val_loss' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "get_accuracy(vit, val_loader, coarse_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def get_accuracy(model, datalaoder, coarse_labels=None) -> float:\n",
    "    if coarse_labels:\n",
    "        coarse_labels_int = {i:cl for i, cl in enumerate(coarse_labels)}\n",
    "    model.eval()\n",
    "    correct, total = 0.0, 0\n",
    "\n",
    "    dataloader_tqdm = tqdm(\n",
    "        datalaoder, \n",
    "        desc=f\"[Validation]\", \n",
    "        leave=False\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels) in enumerate(dataloader_tqdm):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            print(labels)\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type):\n",
    "                outputs, _ = model(images)\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            batch_correct = predicted.eq(labels).sum().item()\n",
    "            batch_total = labels.size(0)\n",
    "\n",
    "            val_loss += batch_loss * batch_total\n",
    "            correct += batch_correct\n",
    "            total += batch_total\n",
    "\n",
    "            global_step = epoch * len(self.train_loader) + len(self.train_loader) + batch_idx\n",
    "            batch_acc = 100.0 * batch_correct / batch_total\n",
    "            self.writer.add_scalar('Loss/val_batch', batch_loss, global_step)\n",
    "            self.writer.add_scalar('Accuracy/val_batch', batch_acc, global_step)\n",
    "\n",
    "            if (batch_idx + 1) % self.log_interval == 0 or (batch_idx + 1) == len(self.val_loader):\n",
    "                cumulative_loss = val_loss / total\n",
    "                cumulative_acc = 100.0 * correct / total\n",
    "                val_loader_tqdm.set_postfix(\n",
    "                    loss=f\"{cumulative_loss:.4f}\", \n",
    "                    accuracy=f\"{cumulative_acc:.2f}%\"\n",
    "                )\n",
    "\n",
    "    epoch_loss = val_loss / total\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
