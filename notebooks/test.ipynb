{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/cv-proj2/notebooks/../src/modules/paths.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingWarmRestarts\n",
    "sys.path.append('../src')\n",
    "from modules import (\n",
    "                    paths,\n",
    "                    dataset,\n",
    "                    model,\n",
    "                    utils,\n",
    "                    acdc,\n",
    "                    train\n",
    "                    )\n",
    "from torchvision.transforms import v2\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = v2.Compose([\n",
    "    v2.Lambda(lambda x: x.convert('RGB')),  # some images are in grayscale\n",
    "    v2.ToImage(), \n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.RandAugment(),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    v2.RandomErasing(p=0.25),\n",
    "\n",
    "])\n",
    "\n",
    "transform_valid = v2.Compose([\n",
    "    v2.Lambda(lambda x: x.convert('RGB')),  # some images are in grayscale\n",
    "    v2.ToImage(), \n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading full dataet\n",
      "Loading animal dataset from /home/user/cv-proj2/notebooks/../data/animal_train.pkl\n",
      "Loading animal dataset from /home/user/cv-proj2/notebooks/../data/animal_valid.pkl\n",
      "train:\n",
      "Dataset({\n",
      "    features: ['image', 'label'],\n",
      "    num_rows: 29000\n",
      "})\n",
      "validation:\n",
      "Dataset({\n",
      "    features: ['image', 'label'],\n",
      "    num_rows: 2900\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(dataset)\n",
    "if toy == True:\n",
    "    print(\"laoding toy datasets\")\n",
    "    train_dataset, coarse_labels = dataset.load_animal_dataset(\"train\", transform=transform_train, tiny=True, stop=6)\n",
    "    val_dataset, coarse_labels = dataset.load_animal_dataset(\"valid\", transform=transform_valid, tiny=True, stop=2)\n",
    "\n",
    "else:\n",
    "    print(\"loading full dataet\")\n",
    "    train_dataset, coarse_labels = dataset.load_animal_dataset(\"train\", transform=transform_train)\n",
    "    val_dataset, coarse_labels = dataset.load_animal_dataset(\"valid\", transform=transform_valid)\n",
    "\n",
    "train_dataset = dataset.TorchDatasetWrapper(train_dataset, transform=transform_train)\n",
    "val_dataset = dataset.TorchDatasetWrapper(val_dataset, transform=transform_valid)\n",
    "print(\"train:\\n\"+str(train_dataset))\n",
    "print(\"validation:\\n\"+str(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5 if toy else 4096 \n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,  \n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2,  \n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4,\n",
    "    persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from checkpoint: /home/user/cv-proj2/notebooks/../checkpoints/checkpoint.pth\n",
      "Checkpoint loaded. Resuming from epoch 494 with best accuracy 26.97%.\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(model)\n",
    "config = {\n",
    "    \"patch_size\": 8,           # Kept small for fine-grained patches\n",
    "    \"hidden_size\": 64,          # Increased from 48 (better representation)\n",
    "    \"num_hidden_layers\": 6,     # Deeper for pruning flexibility\n",
    "    \"num_attention_heads\": 8,   # More heads (head_dim = 64/8 = 8)\n",
    "    \"intermediate_size\": 4 * 64,# Standard FFN scaling\n",
    "    \"hidden_dropout_prob\": 0.2, # Mild dropout for regularization\n",
    "    \"attention_probs_dropout_prob\": 0.2,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"image_size\": 64,\n",
    "    \"num_classes\": 58,\n",
    "    \"num_channels\": 3,\n",
    "    \"qkv_bias\": True,           # Keep bias for now (can prune later)\n",
    "}\n",
    "\n",
    "importlib.reload(train)\n",
    "\n",
    "class SoftTargetCrossEntropy(nn.Module):\n",
    "    \"\"\"Cross-entropy loss compatible with Mixup/Cutmix soft labels\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x, target):\n",
    "        # x = model outputs (logits)\n",
    "        # target = mixed labels (probability distributions)\n",
    "        loss = torch.sum(-target * F.log_softmax(x, dim=1), dim=1)\n",
    "        return loss.mean()\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "vit = model.ViT(config).to(device)\n",
    "\n",
    "num_epochs = 500\n",
    "warmup_epochs = 20\n",
    "base_lr = 3e-4\n",
    "min_lr = 1e-6\n",
    "weight_decay = 0.05  # For AdamW optimizer\n",
    "label_smoothing = 0.1  # For cross-entropy\n",
    "patience = 50\n",
    "\n",
    "\n",
    "\n",
    "optimizer = AdamW(vit.parameters(),\n",
    "                  lr=base_lr,\n",
    "                  weight_decay = weight_decay,\n",
    "                  betas=(0.9, 0.98),\n",
    "                  eps = 1e-6      \n",
    "                  )\n",
    "\n",
    "# Linear warmup for 30 epochs (0 → base_lr)\n",
    "warmup = LinearLR(\n",
    "    optimizer,\n",
    "    start_factor=1e-6,  # Near-zero initial LR\n",
    "    end_factor=1.0,     # Full LR after warmup\n",
    "    total_iters=warmup_epochs,\n",
    ")\n",
    "\n",
    "cosine = CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=num_epochs - warmup_epochs,  \n",
    "    eta_min=min_lr,\n",
    ")\n",
    "\n",
    "# Combine them\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[warmup, cosine],\n",
    "    milestones=[warmup_epochs], \n",
    ")\n",
    "\n",
    "mixup_fn = v2.MixUp(\n",
    "    alpha=1.0,          \n",
    "    num_classes=58\n",
    ")\n",
    "\n",
    "trainer = train.Trainer(model=vit,\n",
    "                        train_loader=train_loader,\n",
    "                        val_loader=val_loader,\n",
    "                        optimizer=optimizer,\n",
    "                        criterion=SoftTargetCrossEntropy(),\n",
    "                        val_criterion=nn.CrossEntropyLoss(),\n",
    "                        scheduler=scheduler,\n",
    "                        device = device,\n",
    "                        writer=torch.utils.tensorboard.SummaryWriter(log_dir=paths.logs),\n",
    "                        scaler=torch.amp.GradScaler(),\n",
    "                        num_epochs=num_epochs,\n",
    "                        log_interval=50,\n",
    "                        model_dir=paths.chekpoints,\n",
    "                        mixup_fn=mixup_fn,\n",
    "                        early_stop_patience=20,\n",
    "                        model_name=\"vit1\",\n",
    "                        resume=True\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing dataset by class for contrastive sampling...\n",
      "Indexing complete.\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(acdc)\n",
    "importlib.reload(dataset)\n",
    "acdc_dataset = dataset.ContrastiveWrapper(val_dataset, coarse_labels)\n",
    "\n",
    "acdc_loader = DataLoader(\n",
    "    acdc_dataset,\n",
    "    batch_size=50,\n",
    "    shuffle=False,\n",
    "    # num_workers=1,  \n",
    "    # pin_memory=False,\n",
    "    # prefetch_factor=1,\n",
    "    collate_fn=dataset.contrastive_collate_fn,\n",
    "    # persistent_workers=False\n",
    ")\n",
    "clean_batch, corrupted_batch = next(iter(acdc_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(acdc)\n",
    "importlib.reload(utils)\n",
    "run_acdc = False\n",
    "if run_acdc:\n",
    "    circuits = {}\n",
    "    for tau in [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "        circuits[str(tau)] = acdc.run_ACDC_optimized(vit, tau, acdc_loader, device=device)\n",
    "else:\n",
    "    cirtcuits_paths = paths.chekpoints / \"circuits.pkl\"\n",
    "    import pickle\n",
    "    circuits = pickle.load(open(cirtcuits_paths, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded. Resuming from epoch 397 with best accuracy 26.97%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model accuracy: 60.06896551724138\n",
      "Checkpoint loaded. Resuming from epoch 397 with best accuracy 26.97%.\n",
      "Pruning 0 unused MLP layers...\n",
      "Pruning 0 unused attention heads...\n",
      "\n",
      "Model pruned. Ready for retraining on the circuit.\n",
      "Testing model with tau 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 60.06896551724138 | Avg Time over 3 runs: 3.1808121744791666 seconds\n",
      "Checkpoint loaded. Resuming from epoch 397 with best accuracy 26.97%.\n",
      "Pruning 0 unused MLP layers...\n",
      "Pruning 0 unused attention heads...\n",
      "\n",
      "Model pruned. Ready for retraining on the circuit.\n",
      "Testing model with tau 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 60.06896551724138 | Avg Time over 3 runs: 3.3248758138020835 seconds\n",
      "Checkpoint loaded. Resuming from epoch 397 with best accuracy 26.97%.\n",
      "Pruning 0 unused MLP layers...\n",
      "Pruning 7 unused attention heads...\n",
      "\n",
      "Model pruned. Ready for retraining on the circuit.\n",
      "Testing model with tau 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 60.172413793103445 | Avg Time over 3 runs: 3.2313787434895835 seconds\n",
      "Checkpoint loaded. Resuming from epoch 397 with best accuracy 26.97%.\n",
      "Pruning 0 unused MLP layers...\n",
      "Pruning 26 unused attention heads...\n",
      "\n",
      "Model pruned. Ready for retraining on the circuit.\n",
      "Testing model with tau 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 49.13793103448276 | Avg Time over 3 runs: 3.204301188151042 seconds\n",
      "Checkpoint loaded. Resuming from epoch 397 with best accuracy 26.97%.\n",
      "Pruning 0 unused MLP layers...\n",
      "Pruning 41 unused attention heads...\n",
      "\n",
      "Model pruned. Ready for retraining on the circuit.\n",
      "Testing model with tau 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 31.82758620689655 | Avg Time over 3 runs: 3.1933037109374998 seconds\n",
      "Checkpoint loaded. Resuming from epoch 397 with best accuracy 26.97%.\n",
      "Pruning 0 unused MLP layers...\n",
      "Pruning 48 unused attention heads...\n",
      "\n",
      "Model pruned. Ready for retraining on the circuit.\n",
      "Testing model with tau 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 6.896551724137931 | Avg Time over 3 runs: 3.2307185058593753 seconds\n",
      "Checkpoint loaded. Resuming from epoch 397 with best accuracy 26.97%.\n",
      "Pruning 3 unused MLP layers...\n",
      "Pruning 48 unused attention heads...\n",
      "\n",
      "Model pruned. Ready for retraining on the circuit.\n",
      "Testing model with tau 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 8.620689655172415 | Avg Time over 3 runs: 3.20207958984375 seconds\n",
      "Checkpoint loaded. Resuming from epoch 397 with best accuracy 26.97%.\n",
      "Pruning 3 unused MLP layers...\n",
      "Pruning 48 unused attention heads...\n",
      "\n",
      "Model pruned. Ready for retraining on the circuit.\n",
      "Testing model with tau 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 8.620689655172415 | Avg Time over 3 runs: 3.2388056640625003 seconds\n",
      "Checkpoint loaded. Resuming from epoch 397 with best accuracy 26.97%.\n",
      "Pruning 3 unused MLP layers...\n",
      "Pruning 48 unused attention heads...\n",
      "\n",
      "Model pruned. Ready for retraining on the circuit.\n",
      "Testing model with tau 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 8.620689655172415 | Avg Time over 3 runs: 3.2212481282552083 seconds\n",
      "Checkpoint loaded. Resuming from epoch 397 with best accuracy 26.97%.\n",
      "Pruning 4 unused MLP layers...\n",
      "Pruning 48 unused attention heads...\n",
      "\n",
      "Model pruned. Ready for retraining on the circuit.\n",
      "Testing model with tau 0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 8.620689655172415 | Avg Time over 3 runs: 3.0961822102864587 seconds\n",
      "Checkpoint loaded. Resuming from epoch 397 with best accuracy 26.97%.\n",
      "Pruning 4 unused MLP layers...\n",
      "Pruning 48 unused attention heads...\n",
      "\n",
      "Model pruned. Ready for retraining on the circuit.\n",
      "Testing model with tau 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 8.620689655172415 | Avg Time over 3 runs: 3.149993408203125 seconds\n",
      "Checkpoint loaded. Resuming from epoch 397 with best accuracy 26.97%.\n",
      "Pruning 4 unused MLP layers...\n",
      "Pruning 48 unused attention heads...\n",
      "\n",
      "Model pruned. Ready for retraining on the circuit.\n",
      "Testing model with tau 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 8.620689655172415 | Avg Time over 3 runs: 3.08873828125 seconds\n",
      "Checkpoint loaded. Resuming from epoch 397 with best accuracy 26.97%.\n",
      "Pruning 4 unused MLP layers...\n",
      "Pruning 48 unused attention heads...\n",
      "\n",
      "Model pruned. Ready for retraining on the circuit.\n",
      "Testing model with tau 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 8.620689655172415 | Avg Time over 3 runs: 3.1799784342447914 seconds\n",
      "Checkpoint loaded. Resuming from epoch 397 with best accuracy 26.97%.\n",
      "Pruning 4 unused MLP layers...\n",
      "Pruning 48 unused attention heads...\n",
      "\n",
      "Model pruned. Ready for retraining on the circuit.\n",
      "Testing model with tau 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 8.620689655172415 | Avg Time over 3 runs: 3.121033121744792 seconds\n",
      "Checkpoint loaded. Resuming from epoch 397 with best accuracy 26.97%.\n",
      "Pruning 4 unused MLP layers...\n",
      "Pruning 48 unused attention heads...\n",
      "\n",
      "Model pruned. Ready for retraining on the circuit.\n",
      "Testing model with tau 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 8.620689655172415 | Avg Time over 3 runs: 3.0938041178385416 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "acdc.test_taus(circuits, val_loader, coarse_labels, config, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(model)\n",
    "vit.classifier = nn.Linear(config[\"hidden_size\"], len(coarse_labels))\n",
    "vit.apply(vit._init_weights)\n",
    "vit = vit.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from checkpoint: /home/user/cv-proj2/notebooks/../checkpoints/checkpoint.pth\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ViT:\n\tsize mismatch for classifier.weight: copying a param with shape torch.Size([58, 64]) from checkpoint, the shape in current model is torch.Size([6, 64]).\n\tsize mismatch for classifier.bias: copying a param with shape torch.Size([58]) from checkpoint, the shape in current model is torch.Size([6]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     23\u001b[39m scheduler = SequentialLR(\n\u001b[32m     24\u001b[39m     optimizer,\n\u001b[32m     25\u001b[39m     schedulers=[warmup, cosine],\n\u001b[32m     26\u001b[39m     milestones=[warmup_epochs], \n\u001b[32m     27\u001b[39m )\n\u001b[32m     29\u001b[39m mixup_fn = v2.MixUp(\n\u001b[32m     30\u001b[39m     alpha=\u001b[32m1.0\u001b[39m,          \n\u001b[32m     31\u001b[39m     num_classes=\u001b[32m58\u001b[39m\n\u001b[32m     32\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m trainer = \u001b[43mtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m                        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSoftTargetCrossEntropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mval_criterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensorboard\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSummaryWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mamp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGradScaler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchekpoints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mmixup_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmixup_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mearly_stop_patience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvit1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mresume\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     51\u001b[39m \u001b[43m                        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cv-proj2/notebooks/../src/modules/train.py:62\u001b[39m, in \u001b[36mTrainer.__init__\u001b[39m\u001b[34m(self, model, train_loader, val_loader, optimizer, criterion, val_criterion, scheduler, writer, device, scaler, num_epochs, log_interval, model_dir, early_stop_patience, checkpoint_interval, mixup_fn, model_name, resume)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(checkpoint_path):\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResuming training from checkpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     64\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResume flag is set, but no checkpoint was found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Starting from scratch.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cv-proj2/notebooks/../src/modules/train.py:86\u001b[39m, in \u001b[36mTrainer.load_checkpoint\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m     84\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     85\u001b[39m             new_state_dict[k] = v\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_state_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.load_state_dict(checkpoint[\u001b[33m'\u001b[39m\u001b[33moptimizer\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     89\u001b[39m \u001b[38;5;28mself\u001b[39m.scheduler.load_state_dict(checkpoint[\u001b[33m'\u001b[39m\u001b[33mscheduler\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:2593\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2585\u001b[39m         error_msgs.insert(\n\u001b[32m   2586\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2587\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2588\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2589\u001b[39m             ),\n\u001b[32m   2590\u001b[39m         )\n\u001b[32m   2592\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2593\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2594\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2595\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2596\u001b[39m         )\n\u001b[32m   2597\u001b[39m     )\n\u001b[32m   2598\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for ViT:\n\tsize mismatch for classifier.weight: copying a param with shape torch.Size([58, 64]) from checkpoint, the shape in current model is torch.Size([6, 64]).\n\tsize mismatch for classifier.bias: copying a param with shape torch.Size([58]) from checkpoint, the shape in current model is torch.Size([6])."
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(vit.parameters(),\n",
    "                  lr=base_lr,\n",
    "                  weight_decay = weight_decay,\n",
    "                  betas=(0.9, 0.98),\n",
    "                  eps = 1e-6      \n",
    "                  )\n",
    "\n",
    "# Linear warmup for 30 epochs (0 → base_lr)\n",
    "warmup = LinearLR(\n",
    "    optimizer,\n",
    "    start_factor=1e-6,  # Near-zero initial LR\n",
    "    end_factor=1.0,     # Full LR after warmup\n",
    "    total_iters=warmup_epochs,\n",
    ")\n",
    "\n",
    "cosine = CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=num_epochs - warmup_epochs,  \n",
    "    eta_min=min_lr,\n",
    ")\n",
    "\n",
    "# Combine them\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[warmup, cosine],\n",
    "    milestones=[warmup_epochs], \n",
    ")\n",
    "\n",
    "mixup_fn = v2.MixUp(\n",
    "    alpha=1.0,          \n",
    "    num_classes=58\n",
    ")\n",
    "\n",
    "trainer = train.Trainer(model=vit,\n",
    "                        train_loader=train_loader,\n",
    "                        val_loader=val_loader,\n",
    "                        optimizer=optimizer,\n",
    "                        criterion=SoftTargetCrossEntropy(),\n",
    "                        val_criterion=nn.CrossEntropyLoss(),\n",
    "                        scheduler=scheduler,\n",
    "                        device = device,\n",
    "                        writer=torch.utils.tensorboard.SummaryWriter(log_dir=paths.logs),\n",
    "                        scaler=torch.amp.GradScaler(),\n",
    "                        num_epochs=num_epochs,\n",
    "                        log_interval=50,\n",
    "                        model_dir=paths.chekpoints,\n",
    "                        mixup_fn=mixup_fn,\n",
    "                        early_stop_patience=20,\n",
    "                        model_name=\"vit1\",\n",
    "                        resume=True\n",
    "                        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
