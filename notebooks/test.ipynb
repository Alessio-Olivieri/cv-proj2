{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingWarmRestarts\n",
    "sys.path.append('../src')\n",
    "from modules import (\n",
    "                    paths,\n",
    "                    dataset,\n",
    "                    model,\n",
    "                    utils,\n",
    "                    acdc,\n",
    "                    train\n",
    "                    )\n",
    "from torchvision.transforms import v2\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = v2.Compose([\n",
    "    v2.Lambda(lambda x: x.convert('RGB')),  # some images are in grayscale\n",
    "    v2.ToImage(), \n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.RandAugment(),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    v2.RandomErasing(p=0.25),\n",
    "\n",
    "])\n",
    "\n",
    "transform_valid = v2.Compose([\n",
    "    v2.Lambda(lambda x: x.convert('RGB')),  # some images are in grayscale\n",
    "    v2.ToImage(), \n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(dataset)\n",
    "if toy == True:\n",
    "    print(\"laoding toy datasets\")\n",
    "    train_dataset, coarse_labels = dataset.load_animal_dataset(\"train\", transform=transform_train, tiny=True, stop=6)\n",
    "    val_dataset, coarse_labels = dataset.load_animal_dataset(\"valid\", transform=transform_valid, tiny=True, stop=2)\n",
    "\n",
    "else:\n",
    "    print(\"loading full dataet\")\n",
    "    train_dataset, coarse_labels = dataset.load_animal_dataset(\"train\", transform=transform_train)\n",
    "    val_dataset, coarse_labels = dataset.load_animal_dataset(\"valid\", transform=transform_valid)\n",
    "\n",
    "train_dataset = dataset.TorchDatasetWrapper(train_dataset, transform=transform_train)\n",
    "val_dataset = dataset.TorchDatasetWrapper(val_dataset, transform=transform_valid)\n",
    "print(\"train:\\n\"+str(train_dataset))\n",
    "print(\"validation:\\n\"+str(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2 if toy else 4096 \n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,  \n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2,  \n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4,\n",
    "    persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(model)\n",
    "config = {\n",
    "    \"patch_size\": 8,           # Kept small for fine-grained patches\n",
    "    \"hidden_size\": 100,          # Increased from 48 (better representation)\n",
    "    \"num_hidden_layers\": 6,     # Deeper for pruning flexibility\n",
    "    \"num_attention_heads\": 8,   # More heads (head_dim = 64/8 = 8)\n",
    "    \"intermediate_size\": 4 * 64,# Standard FFN scaling\n",
    "    \"hidden_dropout_prob\": 0.1, # Mild dropout for regularization\n",
    "    \"attention_probs_dropout_prob\": 0.1,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"image_size\": 64,\n",
    "    \"num_classes\": 200,\n",
    "    \"num_channels\": 3,\n",
    "    \"qkv_bias\": True,           # Keep bias for now (can prune later)\n",
    "}\n",
    "\n",
    "importlib.reload(train)\n",
    "\n",
    "class SoftTargetCrossEntropy(nn.Module):\n",
    "    \"\"\"Cross-entropy loss compatible with Mixup/Cutmix soft labels\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x, target):\n",
    "        # x = model outputs (logits)\n",
    "        # target = mixed labels (probability distributions)\n",
    "        loss = torch.sum(-target * F.log_softmax(x, dim=1), dim=1)\n",
    "        return loss.mean()\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "vit = model.ViT(config).to(device)\n",
    "\n",
    "# num_epochs = 200\n",
    "# warmup_epochs = 2\n",
    "# base_lr = 3e-4\n",
    "# min_lr = 1e-6\n",
    "# weight_decay = 0.05  # For AdamW optimizer\n",
    "# label_smoothing = 0.1  # For cross-entropy\n",
    "# patience = 20\n",
    "\n",
    "\n",
    "\n",
    "# optimizer = AdamW(vit.parameters(),\n",
    "#                   lr=base_lr,\n",
    "#                   weight_decay = weight_decay,\n",
    "#                   betas=(0.9, 0.999)\n",
    "#                   )\n",
    "\n",
    "# # Linear warmup for 30 epochs (0 â†’ base_lr)\n",
    "# warmup = LinearLR(\n",
    "#     optimizer,\n",
    "#     start_factor=1e-6,  # Near-zero initial LR\n",
    "#     end_factor=1.0,     # Full LR after warmup\n",
    "#     total_iters=warmup_epochs,\n",
    "# )\n",
    "\n",
    "# # Cosine decay for remaining epochs (170)\n",
    "# cosine = CosineAnnealingWarmRestarts(\n",
    "#     optimizer,\n",
    "#     T_0=num_epochs - warmup_epochs,  # 170 epochs per cycle\n",
    "#     eta_min=min_lr,\n",
    "# )\n",
    "\n",
    "# # Combine them\n",
    "# scheduler = SequentialLR(\n",
    "#     optimizer,\n",
    "#     schedulers=[warmup, cosine],\n",
    "#     milestones=[warmup_epochs],  # Switch after warmup\n",
    "# )\n",
    "\n",
    "# mixup_fn = v2.MixUp(\n",
    "#     alpha=1.0,          # Add CutMix\n",
    "#     num_classes=200\n",
    "# )\n",
    "\n",
    "# trainer = train.Trainer(model=vit,\n",
    "#                         train_loader=train_loader,\n",
    "#                         val_loader=val_loader,\n",
    "#                         optimizer=optimizer,\n",
    "#                         criterion=SoftTargetCrossEntropy(),\n",
    "#                         val_criterion=nn.CrossEntropyLoss(),\n",
    "#                         scheduler=scheduler,\n",
    "#                         device = device,\n",
    "#                         writer=torch.utils.tensorboard.SummaryWriter(log_dir=paths.logs),\n",
    "#                         scaler=torch.amp.GradScaler(),\n",
    "#                         num_epochs=num_epochs,\n",
    "#                         log_interval=50,\n",
    "#                         model_dir=paths.chekpoints,\n",
    "#                         mixup_fn=mixup_fn,\n",
    "#                         early_stop_patience=20,\n",
    "#                         model_name=\"vit1\",\n",
    "#                         resume=True\n",
    "#                         )\n",
    "# acc = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler = CosineAnnealingWarmRestarts(\n",
    "#     optimizer,\n",
    "#     T_0=200,  # A new 100-epoch decay cycle\n",
    "#     eta_min=min_lr,\n",
    "# )\n",
    "# num_epochs = 400\n",
    "# trainer = train.Trainer(model=vit,\n",
    "#                         train_loader=train_loader,\n",
    "#                         val_loader=val_loader,\n",
    "#                         optimizer=optimizer,\n",
    "#                         criterion=SoftTargetCrossEntropy(),\n",
    "#                         val_criterion=nn.CrossEntropyLoss(),\n",
    "#                         scheduler=scheduler,\n",
    "#                         device = device,\n",
    "#                         writer=torch.utils.tensorboard.SummaryWriter(log_dir=paths.logs),\n",
    "#                         scaler=torch.amp.GradScaler(),\n",
    "#                         num_epochs=num_epochs,\n",
    "#                         log_interval=50,\n",
    "#                         model_dir=paths.chekpoints,\n",
    "#                         mixup_fn=mixup_fn,\n",
    "#                         early_stop_patience=20,\n",
    "#                         model_name=\"vit1\",\n",
    "#                         resume=True\n",
    "#                         )\n",
    "# acc = trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(acdc)\n",
    "importlib.reload(dataset)\n",
    "acdc_dataset = dataset.ContrastiveWrapper(train_dataset, coarse_labels)\n",
    "\n",
    "acdc_loader = DataLoader(\n",
    "    acdc_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    # num_workers=1,  \n",
    "    # pin_memory=False,\n",
    "    # prefetch_factor=1,\n",
    "    collate_fn=dataset.contrastive_collate_fn,\n",
    "    # persistent_workers=False\n",
    ")\n",
    "clean_batch, corrupted_batch = next(iter(acdc_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(acdc)\n",
    "importlib.reload(utils)\n",
    "\n",
    "acdc.run_ACDC(vit, 0.4, acdc_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
