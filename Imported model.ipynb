{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46eca436-0a28-406a-8836-af84198ef131",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pillow -q\n",
    "!pip install datasets -q\n",
    "!pip install torch -q\n",
    "!pip install torchvision -q\n",
    "!pip install matplotlib -q\n",
    "!pip install datasets -q\n",
    "!pip install torchmetrics -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5cbaaa0-46f0-4211-aa7a-8b3a4ae78297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import PIL\n",
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb95892-d819-4bfa-a632-828bd46d2efe",
   "metadata": {},
   "source": [
    "# Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21e6977e-1180-4ba4-bde8-6cd481de2172",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/\"\n",
    "MODEL_DIRS = DATA_DIR+\"models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec2cd04-77f8-4563-9ce7-5befb5e72955",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b71f3609-35e8-4aa6-aba7-c9aef1ca5d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchDatasetWrapper(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset, transform=None):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.transform=transform\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.hf_dataset)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.hf_dataset[idx]\n",
    "        image = example['image']\n",
    "        if self.transform:\n",
    "            image =  self.transform(image)\n",
    "        return image, example['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5da5e7-d745-4b13-a6f0-0d528111702a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f7290ad-89de-4997-aca6-f01ccc4e5c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_patches(image_unfolded):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from torchvision.utils import make_grid\n",
    "    if len(image_unfolded.shape) == 5:\n",
    "        image_unfolded = image_unfolded[None,:,:,:,:,:]\n",
    "    for i in range(image_unfolded.shape[0]):\n",
    "        n_channels, n_patches_h, n_patches_w, h, w = image_unfolded.shape[1:]\n",
    "        patches = image_unfolded[i].permute(1, 2, 0, 3, 4).reshape(-1, n_channels, h, w)\n",
    "        grid = make_grid(patches, nrow=n_patches_w, padding=2, normalize=True)\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        plt.imshow(grid.permute(1, 2, 0).numpy())\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acb90e6c-b07c-45d2-9a64-0ea8aac57c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_super_tiny(dataset, split, q=10, p=0, step=500, classes=200):\n",
    "    from datasets import Dataset\n",
    "    images_per_class = {\"validation\":50,\n",
    "                       \"test\":50,\n",
    "                       \"train\":500\n",
    "                       }\n",
    "    dataset_length = images_per_class[split]*classes\n",
    "    t = [dataset[p+i:i+q] for i in range(0, dataset_length-q, step)]\n",
    "    all_images = [image for image_class_dict in t for image in image_class_dict[\"image\"]]\n",
    "    all_labels = [image for image_class_dict in t for image in image_class_dict[\"label\"]]\n",
    "    return Dataset.from_dict({\"image\":all_images, \"label\":all_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb2a5b20-71ba-4133-8f48-46432f1c3b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = v2.Compose([\n",
    "    v2.Lambda(lambda x: x.convert('RGB')),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "train_transform = v2.Compose([\n",
    "    v2.Lambda(lambda x: x.convert('RGB')),\n",
    "    v2.RandAugment(),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "train_dataset = TorchDatasetWrapper(load_dataset('Maysee/tiny-imagenet', split='train'),\n",
    "                                    transform = train_transform)\n",
    "valid_dataset = TorchDatasetWrapper(load_dataset('Maysee/tiny-imagenet', split='valid'),\n",
    "                                    transform = val_transform)\n",
    "train_batch_size = 900\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "val_batch_size = 900\n",
    "val_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=val_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef67ca1-a10f-469c-b42e-5313d43328bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
