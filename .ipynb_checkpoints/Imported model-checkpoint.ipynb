{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46eca436-0a28-406a-8836-af84198ef131",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pillow -q\n",
    "!pip install datasets -q\n",
    "!pip install torch -q\n",
    "!pip install torchvision -q\n",
    "!pip install matplotlib -q\n",
    "!pip install datasets -q\n",
    "!pip install torchmetrics -q\n",
    "!pip install timm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5cbaaa0-46f0-4211-aa7a-8b3a4ae78297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 23:53:17.806062: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747525997.848639   11533 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747525997.860878   11533 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-17 23:53:17.901372: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import PIL\n",
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import os\n",
    "import timm\n",
    "from timm.loss import SoftTargetCrossEntropy\n",
    "import pickle\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb95892-d819-4bfa-a632-828bd46d2efe",
   "metadata": {},
   "source": [
    "# Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c603faa5-4728-4fcd-a087-9bd2d85b4082",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "NUM_CLASSES = 200\n",
    "BATCH_SIZE = 64  \n",
    "NUM_EPOCHS = 10  \n",
    "LEARNING_RATE = 1e-4  \n",
    "WEIGHT_DECAY = 0.01  \n",
    "IMAGE_SIZE = 384\n",
    "LOG_INTERVAL = 100 \n",
    "DATA_DIR = \"data/\"\n",
    "MODEL_DIRS = DATA_DIR+\"models/\"\n",
    "TRAIN_DATA = DATA_DIR+\"train.pkl\"\n",
    "VALID_DATA = DATA_DIR+\"valid.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec2cd04-77f8-4563-9ce7-5befb5e72955",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b71f3609-35e8-4aa6-aba7-c9aef1ca5d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchDatasetWrapper(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset, transform=None):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.transform=transform\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.hf_dataset)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.hf_dataset[idx]\n",
    "        image = example['image']\n",
    "        if self.transform:\n",
    "            image =  self.transform(image)\n",
    "        return image, example['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3d206d0-9a4f-4659-bac6-9e7230608f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_super_tiny(dataset, split, q=10, p=0, step=500, classes=200):\n",
    "    from datasets import Dataset\n",
    "    images_per_class = {\"validation\":50,\n",
    "                       \"test\":50,\n",
    "                       \"train\":500\n",
    "                       }\n",
    "    dataset_length = images_per_class[split]*classes\n",
    "    t = [dataset[p+i:i+q] for i in range(0, dataset_length-q, step)]\n",
    "    all_images = [image for image_class_dict in t for image in image_class_dict[\"image\"]]\n",
    "    all_labels = [image for image_class_dict in t for image in image_class_dict[\"label\"]]\n",
    "    return Dataset.from_dict({\"image\":all_images, \"label\":all_labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0791ca8-108e-4019-93e6-96377f173a9a",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e91ab4-29ce-4b2a-b89f-72ac6147a628",
   "metadata": {},
   "source": [
    "Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2b8b7d1-3f13-46ba-b76f-c30dc15e6b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    train_dataset = pickle.load(open(TRAIN_DATA,\"rb\"))\n",
    "    valid_dataset = pickle.load(open(VALID_DATA,\"rb\"))\n",
    "except:\n",
    "    train_dataset = load_dataset('Maysee/tiny-imagenet', split='train')\n",
    "    valid_dataset = load_dataset('Maysee/tiny-imagenet', split='valid')\n",
    "    pickle.dump(train_dataset, open(TRAIN_DATA,\"wb\"))\n",
    "    pickle.dump(valid_dataset, open(VALID_DATA,\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a43dfb-ea56-4afe-ad16-f7676c0870f9",
   "metadata": {},
   "source": [
    "#### Data Augmentation and Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90ffe13b-7bb5-4c32-b804-288f7e7a1cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = v2.Compose([\n",
    "    v2.Lambda(lambda x: x.convert('RGB')),  # some images are in grayscale\n",
    "    v2.Resize((IMAGE_SIZE, IMAGE_SIZE), interpolation=v2.InterpolationMode.BICUBIC),\n",
    "    v2.ToImage(), \n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.RandAugment(),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    v2.RandomErasing(p=0.25),\n",
    "\n",
    "])\n",
    "\n",
    "transform_valid = v2.Compose([\n",
    "    v2.Lambda(lambda x: x.convert('RGB')),  # some images are in grayscale\n",
    "    v2.Resize((IMAGE_SIZE, IMAGE_SIZE), interpolation=v2.InterpolationMode.BICUBIC),\n",
    "    v2.ToImage(), \n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cf77fc6-284b-4d28-8c1b-569e78d3c47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[ 2.0605,  2.0605,  2.0605,  ...,  2.2318,  2.2318,  2.2318],\n",
       "           [ 2.0777,  2.0777,  2.0777,  ...,  2.2318,  2.2318,  2.2318],\n",
       "           [ 2.0777,  2.0777,  2.0777,  ...,  2.2318,  2.2318,  2.2318],\n",
       "           ...,\n",
       "           [ 2.0092,  2.0092,  2.0092,  ...,  2.1975,  2.1975,  2.1975],\n",
       "           [ 2.0092,  2.0092,  2.0092,  ...,  2.2147,  2.2147,  2.2147],\n",
       "           [ 2.0092,  2.0092,  2.0092,  ...,  2.2147,  2.2318,  2.2318]],\n",
       " \n",
       "          [[ 2.4111,  2.4111,  2.4111,  ...,  2.4111,  2.4111,  2.4111],\n",
       "           [ 2.4111,  2.4111,  2.4111,  ...,  2.4111,  2.4111,  2.4111],\n",
       "           [ 2.4111,  2.4111,  2.4111,  ...,  2.4111,  2.4111,  2.4111],\n",
       "           ...,\n",
       "           [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],\n",
       "           [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],\n",
       "           [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286]],\n",
       " \n",
       "          [[ 2.5703,  2.5703,  2.5703,  ...,  2.6226,  2.6226,  2.6226],\n",
       "           [ 2.5703,  2.5703,  2.5703,  ...,  2.6226,  2.6226,  2.6226],\n",
       "           [ 2.5703,  2.5703,  2.5703,  ...,  2.6226,  2.6226,  2.6226],\n",
       "           ...,\n",
       "           [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],\n",
       "           [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],\n",
       "           [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400]]],\n",
       " \n",
       " \n",
       "         [[[ 0.6906,  0.6906,  0.6906,  ...,  0.5707,  0.5707,  0.5707],\n",
       "           [ 0.6906,  0.6906,  0.6906,  ...,  0.5707,  0.5707,  0.5707],\n",
       "           [ 0.6906,  0.6906,  0.6906,  ...,  0.5878,  0.5878,  0.5878],\n",
       "           ...,\n",
       "           [ 0.2624,  0.2624,  0.2624,  ...,  0.2111,  0.1939,  0.1939],\n",
       "           [ 0.2624,  0.2624,  0.2624,  ...,  0.2111,  0.1939,  0.1939],\n",
       "           [ 0.2453,  0.2453,  0.2453,  ...,  0.2111,  0.1939,  0.1939]],\n",
       " \n",
       "          [[ 1.0630,  1.0630,  1.0630,  ...,  0.9055,  0.9055,  0.9055],\n",
       "           [ 1.0630,  1.0630,  1.0630,  ...,  0.9055,  0.9055,  0.9055],\n",
       "           [ 1.0630,  1.0630,  1.0630,  ...,  0.9230,  0.9230,  0.9230],\n",
       "           ...,\n",
       "           [ 0.6078,  0.6078,  0.6078,  ...,  0.6254,  0.6254,  0.6078],\n",
       "           [ 0.6078,  0.6078,  0.6078,  ...,  0.6429,  0.6429,  0.6254],\n",
       "           [ 0.5903,  0.5903,  0.5903,  ...,  0.6429,  0.6429,  0.6254]],\n",
       " \n",
       "          [[ 1.1237,  1.1237,  1.1237,  ...,  0.8274,  0.8274,  0.8274],\n",
       "           [ 1.1237,  1.1237,  1.1237,  ...,  0.8274,  0.8274,  0.8274],\n",
       "           [ 1.1237,  1.1237,  1.1237,  ...,  0.8448,  0.8448,  0.8448],\n",
       "           ...,\n",
       "           [ 0.4439,  0.4439,  0.4439,  ...,  0.6531,  0.6531,  0.6356],\n",
       "           [ 0.4439,  0.4439,  0.4439,  ...,  0.6531,  0.6531,  0.6356],\n",
       "           [ 0.4265,  0.4265,  0.4265,  ...,  0.6705,  0.6705,  0.6531]]],\n",
       " \n",
       " \n",
       "         [[[-1.7240, -1.7240, -1.7240,  ..., -1.6555, -1.6727, -1.6898],\n",
       "           [-1.7240, -1.7240, -1.7240,  ..., -1.6384, -1.6555, -1.6898],\n",
       "           [-1.7240, -1.7240, -1.7240,  ..., -1.6384, -1.6555, -1.6727],\n",
       "           ...,\n",
       "           [-1.1589, -1.1589, -1.1589,  ..., -1.4843, -1.4843, -1.4843],\n",
       "           [-1.1418, -1.1418, -1.1418,  ..., -1.5014, -1.5014, -1.5014],\n",
       "           [-1.1247, -1.1247, -1.1247,  ..., -1.5014, -1.5014, -1.5014]],\n",
       " \n",
       "          [[-1.6681, -1.6681, -1.6681,  ..., -1.6155, -1.6331, -1.6506],\n",
       "           [-1.6681, -1.6681, -1.6681,  ..., -1.5980, -1.6155, -1.6506],\n",
       "           [-1.6681, -1.6681, -1.6681,  ..., -1.5980, -1.6155, -1.6331],\n",
       "           ...,\n",
       "           [-0.7402, -0.7402, -0.7402,  ..., -1.0028, -1.0028, -1.0028],\n",
       "           [-0.7227, -0.7227, -0.7227,  ..., -1.0203, -1.0203, -1.0203],\n",
       "           [-0.7052, -0.7052, -0.7052,  ..., -1.0203, -1.0203, -1.0203]],\n",
       " \n",
       "          [[-1.0550, -1.0550, -1.0550,  ..., -1.0550, -1.0724, -1.0898],\n",
       "           [-1.0550, -1.0550, -1.0550,  ..., -1.0376, -1.0550, -1.0898],\n",
       "           [-1.0550, -1.0550, -1.0550,  ..., -1.0376, -1.0550, -1.0724],\n",
       "           ...,\n",
       "           [ 0.1476,  0.1476,  0.1476,  ..., -0.1312, -0.1312, -0.1312],\n",
       "           [ 0.1651,  0.1651,  0.1651,  ..., -0.1487, -0.1487, -0.1487],\n",
       "           [ 0.1825,  0.1825,  0.1825,  ..., -0.1487, -0.1487, -0.1487]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-1.2103, -1.2103, -1.2274,  ...,  0.3994,  0.3994,  0.4166],\n",
       "           [-1.2274, -1.2274, -1.2445,  ...,  0.3823,  0.3994,  0.4166],\n",
       "           [-1.2274, -1.2274, -1.2445,  ...,  0.3823,  0.3823,  0.3994],\n",
       "           ...,\n",
       "           [ 0.4679,  0.4508,  0.4508,  ...,  0.8961,  0.9132,  0.9132],\n",
       "           [ 0.4851,  0.4679,  0.4679,  ...,  0.9817,  0.9988,  0.9988],\n",
       "           [ 0.5022,  0.4851,  0.4851,  ...,  1.0331,  1.0673,  1.0673]],\n",
       " \n",
       "          [[-1.1604, -1.1779, -1.1779,  ...,  0.5028,  0.5203,  0.5203],\n",
       "           [-1.1604, -1.1954, -1.1954,  ...,  0.5028,  0.5028,  0.5203],\n",
       "           [-1.1779, -1.1954, -1.1954,  ...,  0.4853,  0.5028,  0.5028],\n",
       "           ...,\n",
       "           [ 0.3452,  0.3452,  0.3277,  ...,  0.6604,  0.6779,  0.6779],\n",
       "           [ 0.3627,  0.3627,  0.3452,  ...,  0.7479,  0.7829,  0.7829],\n",
       "           [ 0.3803,  0.3803,  0.3452,  ...,  0.8179,  0.8354,  0.8354]],\n",
       " \n",
       "          [[-1.8044, -1.8044, -1.8044,  ..., -0.6367, -0.6367, -0.6367],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -0.6541, -0.6367, -0.6367],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -0.6541, -0.6367, -0.6367],\n",
       "           ...,\n",
       "           [-0.5147, -0.5321, -0.5495,  ...,  0.4788,  0.4788,  0.4962],\n",
       "           [-0.4973, -0.5147, -0.5321,  ...,  0.5834,  0.5834,  0.6008],\n",
       "           [-0.4798, -0.4973, -0.5147,  ...,  0.6356,  0.6356,  0.6531]]],\n",
       " \n",
       " \n",
       "         [[[-1.2959, -1.2959, -1.3130,  ..., -0.5767, -0.5938, -0.5938],\n",
       "           [-1.2788, -1.2959, -1.2959,  ..., -0.5938, -0.6109, -0.6109],\n",
       "           [-1.2445, -1.2617, -1.2788,  ..., -0.6281, -0.6452, -0.6452],\n",
       "           ...,\n",
       "           [ 0.6392,  0.6049,  0.5364,  ...,  0.7248,  0.7077,  0.7077],\n",
       "           [ 0.6563,  0.6221,  0.5536,  ...,  0.6906,  0.6734,  0.6734],\n",
       "           [ 0.6563,  0.6221,  0.5536,  ...,  0.6734,  0.6563,  0.6563]],\n",
       " \n",
       "          [[-0.3725, -0.3901, -0.4251,  ..., -1.0728, -1.0903, -1.0903],\n",
       "           [-0.3550, -0.3725, -0.4076,  ..., -1.0728, -1.0903, -1.0903],\n",
       "           [-0.3375, -0.3550, -0.3901,  ..., -1.0903, -1.1078, -1.1078],\n",
       "           ...,\n",
       "           [ 0.0126, -0.0224, -0.0749,  ...,  0.5903,  0.5728,  0.5728],\n",
       "           [ 0.0301, -0.0049, -0.0574,  ...,  0.5553,  0.5378,  0.5378],\n",
       "           [ 0.0301, -0.0049, -0.0574,  ...,  0.5378,  0.5203,  0.5203]],\n",
       " \n",
       "          [[-1.6127, -1.6302, -1.6127,  ..., -0.9504, -0.9678, -0.9678],\n",
       "           [-1.5953, -1.5953, -1.5953,  ..., -0.9678, -0.9853, -0.9853],\n",
       "           [-1.5430, -1.5604, -1.5604,  ..., -0.9853, -1.0027, -1.0027],\n",
       "           ...,\n",
       "           [-0.1138, -0.1312, -0.2010,  ...,  0.8099,  0.8099,  0.7925],\n",
       "           [-0.0964, -0.1138, -0.1835,  ...,  0.7751,  0.7751,  0.7576],\n",
       "           [-0.0964, -0.1138, -0.1835,  ...,  0.7576,  0.7576,  0.7402]]],\n",
       " \n",
       " \n",
       "         [[[-1.9124, -1.9295, -1.9295,  ..., -0.3027, -0.2856, -0.2684],\n",
       "           [-1.8953, -1.9124, -1.9124,  ..., -0.3027, -0.2856, -0.2684],\n",
       "           [-1.8610, -1.8782, -1.8782,  ..., -0.3027, -0.2856, -0.2684],\n",
       "           ...,\n",
       "           [-1.0219, -1.0562, -1.1075,  ..., -0.1143, -0.2171, -0.2856],\n",
       "           [-1.0562, -1.1075, -1.1589,  ..., -0.1828, -0.2856, -0.3541],\n",
       "           [-1.0904, -1.1247, -1.1760,  ..., -0.2171, -0.3198, -0.4054]],\n",
       " \n",
       "          [[-1.2654, -1.2654, -1.2829,  ..., -0.0399, -0.0399, -0.0224],\n",
       "           [-1.2479, -1.2479, -1.2654,  ..., -0.0399, -0.0399, -0.0224],\n",
       "           [-1.2129, -1.2129, -1.2304,  ..., -0.0399, -0.0399, -0.0224],\n",
       "           ...,\n",
       "           [-0.8452, -0.8803, -0.9328,  ..., -0.0924, -0.1975, -0.2675],\n",
       "           [-0.8978, -0.9328, -0.9853,  ..., -0.1625, -0.2675, -0.3375],\n",
       "           [-0.9153, -0.9503, -1.0028,  ..., -0.1975, -0.3025, -0.3901]],\n",
       " \n",
       "          [[-1.8044, -1.8044, -1.8044,  ..., -0.6193, -0.6193, -0.6193],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -0.6018, -0.6018, -0.6018],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -0.5844, -0.5844, -0.5844],\n",
       "           ...,\n",
       "           [-1.1421, -1.1596, -1.1944,  ..., -0.1138, -0.2184, -0.2707],\n",
       "           [-1.1944, -1.2119, -1.2467,  ..., -0.1661, -0.2881, -0.3404],\n",
       "           [-1.2119, -1.2293, -1.2641,  ..., -0.2184, -0.3230, -0.3927]]]]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    TorchDatasetWrapper(train_dataset, transform_train),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,  \n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    TorchDatasetWrapper(valid_dataset, transform_valid),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,  \n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4,\n",
    "    persistent_workers=True\n",
    ")\n",
    "next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "750ec524-1840-44aa-8621-cf13e2363c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixup and CutMix\n",
    "mixup_fn = timm.data.Mixup(\n",
    "    mixup_alpha=0.8,\n",
    "    cutmix_alpha=1.0,\n",
    "    cutmix_minmax=None,\n",
    "    prob=0.5,  # Reduced probability to allow some original images\n",
    "    switch_prob=0.5,\n",
    "    mode='batch',\n",
    "    label_smoothing=0.1,\n",
    "    num_classes=NUM_CLASSES\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac840411-2f6b-4e82-be70-b67e695b3a2e",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2032f2c3-8c27-4df0-ae6e-aec115cc30f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model('vit_base_patch16_384', pretrained=True, num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e17601fd-874b-40b0-a3b5-ae086563e697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=768, out_features=200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32b47a36-6447-4140-a137-4b434f63e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03f49e77-8e5c-43d1-98ca-bd8bbb759540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e5c73d6-030b-4daf-be73-8b9ff9032bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss, Optimizer, and Scheduler\n",
    "criterion = SoftTargetCrossEntropy()  # For Mixup and CutMix\n",
    "\n",
    "# Using SGD with momentum for better fine-tuning\n",
    "optimizer = torch.optim.SGD(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=LEARNING_RATE,\n",
    "    momentum=0.9,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# Scheduler adjusted to steps per epoch\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "\n",
    "# Initialize AMP scaler for mixed precision\n",
    "scaler = torch.amp.GradScaler(device='cuda')  # Updated instantiation\n",
    "\n",
    "# Training and Validation Loop\n",
    "writer = SummaryWriter()  # For TensorBoard logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27549381-fbf7-48ed-b160-7eb6c02d8dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    # Progress bar for training loop\n",
    "    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Training]\", leave=False)\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader_tqdm):\n",
    "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "        # Apply Mixup/CutMix\n",
    "        images, labels = mixup_fn(images, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Calculate batch metrics\n",
    "        batch_loss = loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        _, targets = labels.max(1)\n",
    "        batch_correct = predicted.eq(targets).sum().item()\n",
    "        batch_total = labels.size(0)\n",
    "        batch_acc = 100. * batch_correct / batch_total\n",
    "\n",
    "        # Accumulate epoch metrics\n",
    "        running_loss += batch_loss * batch_total\n",
    "        correct += batch_correct\n",
    "        total += batch_total\n",
    "\n",
    "        # Calculate global step for consistent logging across epochs\n",
    "        global_step = epoch * len(train_loader) + batch_idx\n",
    "\n",
    "        # Log per-batch metrics\n",
    "        writer.add_scalar('Loss/train_batch', batch_loss, global_step)\n",
    "        writer.add_scalar('Accuracy/train_batch', batch_acc, global_step)\n",
    "\n",
    "        # Update progress bar with cumulative metrics\n",
    "        if (batch_idx + 1) % LOG_INTERVAL == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            cumulative_loss = running_loss / total\n",
    "            cumulative_acc = 100. * correct / total\n",
    "            train_loader_tqdm.set_postfix(\n",
    "                loss=f\"{cumulative_loss:.4f}\", \n",
    "                accuracy=f\"{cumulative_acc:.2f}%\"\n",
    "            )\n",
    "\n",
    "    # Calculate and log epoch metrics\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', epoch_acc, epoch)\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e04d0f9f-5439-4d10-b40d-8ab56d3ad395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch):\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    # Progress bar for validation loop\n",
    "    val_loader_tqdm = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Validation]\", leave=False)\n",
    "    criterion_val = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels) in enumerate(val_loader_tqdm):\n",
    "            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion_val(outputs, labels)\n",
    "\n",
    "            # Calculate batch metrics\n",
    "            batch_loss = loss.item()\n",
    "            batch_total = labels.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            batch_correct = predicted.eq(labels).sum().item()\n",
    "            batch_acc = 100. * batch_correct / batch_total\n",
    "\n",
    "            # Accumulate epoch metrics\n",
    "            val_loss += batch_loss * batch_total\n",
    "            correct += batch_correct\n",
    "            total += batch_total\n",
    "\n",
    "            # Create unique global step aligned with training progression\n",
    "            global_step = epoch * len(train_loader) + len(train_loader) + batch_idx\n",
    "\n",
    "            # Log per-batch metrics\n",
    "            writer.add_scalar('Loss/val_batch', batch_loss, global_step)\n",
    "            writer.add_scalar('Accuracy/val_batch', batch_acc, global_step)\n",
    "\n",
    "            # Update progress bar with cumulative metrics\n",
    "            if (batch_idx + 1) % LOG_INTERVAL == 0 or (batch_idx + 1) == len(val_loader):\n",
    "                cumulative_loss = val_loss / total\n",
    "                cumulative_acc = 100. * correct / total\n",
    "                val_loader_tqdm.set_postfix(\n",
    "                    loss=f\"{cumulative_loss:.4f}\", \n",
    "                    accuracy=f\"{cumulative_acc:.2f}%\"\n",
    "                )\n",
    "\n",
    "    # Log epoch metrics\n",
    "    epoch_loss = val_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    writer.add_scalar('Loss/val', epoch_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/val', epoch_acc, epoch)\n",
    "    print(f\"Validation Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.2f}%\")\n",
    "\n",
    "    return epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596fbbab-788d-450a-9e3c-cd1e0451d006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging in runs/May17_18-07-23_673bab8e-c356-488b-8ab0-301a9c7a07aa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 [Training]:   0%|                                                                     | 0/1563 [00:00<?, ?it/s]/tmp/ipykernel_9205/2629270786.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 3.8136, Acc: 35.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 [Validation]:   0%|                                                                    | 0/157 [00:00<?, ?it/s]/tmp/ipykernel_9205/3459858977.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7252, Acc: 85.04%\n",
      "New best model saved with accuracy: 85.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 [Training]:  19%|█████▌                       | 297/1563 [07:05<29:44,  1.41s/it, accuracy=72.45%, loss=2.3200]"
     ]
    }
   ],
   "source": [
    "# Main Training Loop\n",
    "best_acc = 0\n",
    "print(f\"logging in {writer.log_dir}\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_one_epoch(epoch)\n",
    "    val_acc = validate(epoch)\n",
    "\n",
    "    # Scheduler step\n",
    "    scheduler.step()\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        os.makedirs(MODEL_DIRS, exist_ok=True)\n",
    "        # If using DataParallel, save the underlying model\n",
    "        if isinstance(model, torch.nn.DataParallel):\n",
    "            torch.save(model.module.state_dict(), MODEL_DIRS + 'base_vit_tiny_imagenet.pth')\n",
    "        else:\n",
    "            torch.save(model.state_dict(), MODEL_DIRS + 'base_vit_tiny_imagenet.pth')\n",
    "        print(f\"New best model saved with accuracy: {best_acc:.2f}%\")\n",
    "\n",
    "print(\"Training complete. Best validation accuracy:\", best_acc)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97face6d-ca1d-425a-8e84-c01d7f81699c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ffe0ab-eff1-4811-bb07-fe0d168ebb87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe133073-d96e-4098-a6f2-e33ef96f67ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
