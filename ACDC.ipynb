{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264fe379-cebd-40b1-a710-231141028237",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/soniajoseph/ViT-Prisma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e515e65-16e5-4751-9fd4-e6223d5eb501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Obtaining file:///home/user/cv-proj2/ViT-Prisma\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pytest>=6.0 in /opt/anaconda3/lib/python3.12/site-packages (from vit-prisma==2.0.0) (7.4.4)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (from vit-prisma==2.0.0) (2.5.1)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from vit-prisma==2.0.0) (1.26.4)\n",
      "Requirement already satisfied: jaxtyping in /home/user/.local/lib/python3.12/site-packages (from vit-prisma==2.0.0) (0.3.2)\n",
      "Requirement already satisfied: einops in /home/user/.local/lib/python3.12/site-packages (from vit-prisma==2.0.0) (0.8.1)\n",
      "Requirement already satisfied: fancy_einsum in /home/user/.local/lib/python3.12/site-packages (from vit-prisma==2.0.0) (0.0.3)\n",
      "Requirement already satisfied: plotly==5.19.0 in /home/user/.local/lib/python3.12/site-packages (from vit-prisma==2.0.0) (5.19.0)\n",
      "Requirement already satisfied: timm in /home/user/.local/lib/python3.12/site-packages (from vit-prisma==2.0.0) (1.0.15)\n",
      "Requirement already satisfied: transformers in /home/user/.local/lib/python3.12/site-packages (from vit-prisma==2.0.0) (4.51.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from vit-prisma==2.0.0) (1.5.1)\n",
      "Requirement already satisfied: datasets in /home/user/.local/lib/python3.12/site-packages (from vit-prisma==2.0.0) (3.6.0)\n",
      "Collecting line_profiler (from vit-prisma==2.0.0)\n",
      "  Downloading line_profiler-4.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (from vit-prisma==2.0.0) (3.9.2)\n",
      "Collecting wandb (from vit-prisma==2.0.0)\n",
      "  Downloading wandb-0.19.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting kaleido (from vit-prisma==2.0.0)\n",
      "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl.metadata (15 kB)\n",
      "Collecting open-clip-torch (from vit-prisma==2.0.0)\n",
      "  Downloading open_clip_torch-2.32.0-py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from plotly==5.19.0->vit-prisma==2.0.0) (8.2.3)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from plotly==5.19.0->vit-prisma==2.0.0) (24.1)\n",
      "Requirement already satisfied: iniconfig in /opt/anaconda3/lib/python3.12/site-packages (from pytest>=6.0->vit-prisma==2.0.0) (1.1.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /opt/anaconda3/lib/python3.12/site-packages (from pytest>=6.0->vit-prisma==2.0.0) (1.0.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from datasets->vit-prisma==2.0.0) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets->vit-prisma==2.0.0) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/user/.local/lib/python3.12/site-packages (from datasets->vit-prisma==2.0.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets->vit-prisma==2.0.0) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/lib/python3.12/site-packages (from datasets->vit-prisma==2.0.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/lib/python3.12/site-packages (from datasets->vit-prisma==2.0.0) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /home/user/.local/lib/python3.12/site-packages (from datasets->vit-prisma==2.0.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/user/.local/lib/python3.12/site-packages (from datasets->vit-prisma==2.0.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->vit-prisma==2.0.0) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/user/.local/lib/python3.12/site-packages (from datasets->vit-prisma==2.0.0) (0.31.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from datasets->vit-prisma==2.0.0) (6.0.1)\n",
      "Requirement already satisfied: wadler-lindig>=0.1.3 in /home/user/.local/lib/python3.12/site-packages (from jaxtyping->vit-prisma==2.0.0) (0.1.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->vit-prisma==2.0.0) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->vit-prisma==2.0.0) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->vit-prisma==2.0.0) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->vit-prisma==2.0.0) (1.4.4)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->vit-prisma==2.0.0) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->vit-prisma==2.0.0) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->vit-prisma==2.0.0) (2.9.0.post0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (from open-clip-torch->vit-prisma==2.0.0) (0.20.1)\n",
      "Requirement already satisfied: regex in /opt/anaconda3/lib/python3.12/site-packages (from open-clip-torch->vit-prisma==2.0.0) (2024.9.11)\n",
      "Collecting ftfy (from open-clip-torch->vit-prisma==2.0.0)\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: safetensors in /home/user/.local/lib/python3.12/site-packages (from open-clip-torch->vit-prisma==2.0.0) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch->vit-prisma==2.0.0) (4.11.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch->vit-prisma==2.0.0) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/user/.local/lib/python3.12/site-packages (from torch->vit-prisma==2.0.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch->vit-prisma==2.0.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch->vit-prisma==2.0.0) (3.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch->vit-prisma==2.0.0) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->vit-prisma==2.0.0) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->vit-prisma==2.0.0) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->vit-prisma==2.0.0) (3.5.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/user/.local/lib/python3.12/site-packages (from transformers->vit-prisma==2.0.0) (0.21.1)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/anaconda3/lib/python3.12/site-packages (from wandb->vit-prisma==2.0.0) (8.1.7)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb->vit-prisma==2.0.0)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from wandb->vit-prisma==2.0.0) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /opt/anaconda3/lib/python3.12/site-packages (from wandb->vit-prisma==2.0.0) (3.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from wandb->vit-prisma==2.0.0) (4.25.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from wandb->vit-prisma==2.0.0) (5.9.0)\n",
      "Requirement already satisfied: pydantic<3 in /opt/anaconda3/lib/python3.12/site-packages (from wandb->vit-prisma==2.0.0) (2.8.2)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb->vit-prisma==2.0.0)\n",
      "  Downloading sentry_sdk-2.29.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setproctitle (from wandb->vit-prisma==2.0.0)\n",
      "  Downloading setproctitle-1.3.6-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb->vit-prisma==2.0.0) (1.16.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->vit-prisma==2.0.0) (3.10.5)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb->vit-prisma==2.0.0) (4.0.7)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3->wandb->vit-prisma==2.0.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3->wandb->vit-prisma==2.0.0) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets->vit-prisma==2.0.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets->vit-prisma==2.0.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets->vit-prisma==2.0.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets->vit-prisma==2.0.0) (2024.8.30)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.12/site-packages (from ftfy->open-clip-torch->vit-prisma==2.0.0) (0.2.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch->vit-prisma==2.0.0) (2.1.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets->vit-prisma==2.0.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets->vit-prisma==2.0.0) (2023.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->vit-prisma==2.0.0) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->vit-prisma==2.0.0) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->vit-prisma==2.0.0) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->vit-prisma==2.0.0) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->vit-prisma==2.0.0) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->vit-prisma==2.0.0) (1.11.0)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->vit-prisma==2.0.0) (4.0.0)\n",
      "Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hDownloading line_profiler-4.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (720 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m720.1/720.1 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading open_clip_torch-2.32.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wandb-0.19.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading sentry_sdk-2.29.1-py2.py3-none-any.whl (341 kB)\n",
      "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Downloading setproctitle-1.3.6-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
      "Installing collected packages: kaleido, setproctitle, sentry-sdk, line_profiler, ftfy, docker-pycreds, wandb, open-clip-torch, vit-prisma\n",
      "\u001b[33m  DEPRECATION: Legacy editable install of vit-prisma==2.0.0 from file:///home/user/cv-proj2/ViT-Prisma (setup.py develop) is deprecated. pip 25.0 will enforce this behaviour change. A possible replacement is to add a pyproject.toml or enable --use-pep517, and use setuptools >= 64. If the resulting installation is not behaving as expected, try using --config-settings editable_mode=compat. Please consult the setuptools documentation for more information. Discussion can be found at https://github.com/pypa/pip/issues/11457\u001b[0m\u001b[33m\n",
      "\u001b[0m  Running setup.py develop for vit-prisma\n",
      "Successfully installed docker-pycreds-0.4.0 ftfy-6.3.1 kaleido-0.2.1 line_profiler-4.2.0 open-clip-torch-2.32.0 sentry-sdk-2.29.1 setproctitle-1.3.6 vit-prisma-2.0.0 wandb-0.19.11\n"
     ]
    }
   ],
   "source": [
    "!pip install -e ViT-Prisma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efc0e8d5-e0cb-4908-967f-e50ba875d287",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting vit_prisma\n",
      "  Downloading vit_prisma-0.1.4-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pytest>=6.0 in /opt/anaconda3/lib/python3.12/site-packages (from vit_prisma) (7.4.4)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (from vit_prisma) (2.5.1)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from vit_prisma) (1.26.4)\n",
      "Collecting jaxtyping (from vit_prisma)\n",
      "  Downloading jaxtyping-0.3.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting einops (from vit_prisma)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting fancy-einsum (from vit_prisma)\n",
      "  Downloading fancy_einsum-0.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting plotly==5.19.0 (from vit_prisma)\n",
      "  Downloading plotly-5.19.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: timm in /home/user/.local/lib/python3.12/site-packages (from vit_prisma) (1.0.15)\n",
      "Collecting transformers (from vit_prisma)\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from vit_prisma) (1.5.1)\n",
      "Requirement already satisfied: datasets in /home/user/.local/lib/python3.12/site-packages (from vit_prisma) (3.6.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from plotly==5.19.0->vit_prisma) (8.2.3)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from plotly==5.19.0->vit_prisma) (24.1)\n",
      "Requirement already satisfied: iniconfig in /opt/anaconda3/lib/python3.12/site-packages (from pytest>=6.0->vit_prisma) (1.1.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /opt/anaconda3/lib/python3.12/site-packages (from pytest>=6.0->vit_prisma) (1.0.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from datasets->vit_prisma) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets->vit_prisma) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/user/.local/lib/python3.12/site-packages (from datasets->vit_prisma) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets->vit_prisma) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/lib/python3.12/site-packages (from datasets->vit_prisma) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/lib/python3.12/site-packages (from datasets->vit_prisma) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /home/user/.local/lib/python3.12/site-packages (from datasets->vit_prisma) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/user/.local/lib/python3.12/site-packages (from datasets->vit_prisma) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->vit_prisma) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/user/.local/lib/python3.12/site-packages (from datasets->vit_prisma) (0.31.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from datasets->vit_prisma) (6.0.1)\n",
      "Collecting wadler-lindig>=0.1.3 (from jaxtyping->vit_prisma)\n",
      "  Downloading wadler_lindig-0.1.6-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->vit_prisma) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->vit_prisma) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->vit_prisma) (3.5.0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (from timm->vit_prisma) (0.20.1)\n",
      "Requirement already satisfied: safetensors in /home/user/.local/lib/python3.12/site-packages (from timm->vit_prisma) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch->vit_prisma) (4.11.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch->vit_prisma) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/user/.local/lib/python3.12/site-packages (from torch->vit_prisma) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch->vit_prisma) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch->vit_prisma) (3.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch->vit_prisma) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers->vit_prisma) (2024.9.11)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers->vit_prisma)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->vit_prisma) (3.10.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets->vit_prisma) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets->vit_prisma) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets->vit_prisma) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets->vit_prisma) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch->vit_prisma) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets->vit_prisma) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets->vit_prisma) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets->vit_prisma) (2023.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision->timm->vit_prisma) (10.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->vit_prisma) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->vit_prisma) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->vit_prisma) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->vit_prisma) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->vit_prisma) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->vit_prisma) (1.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets->vit_prisma) (1.16.0)\n",
      "Downloading vit_prisma-0.1.4-py3-none-any.whl (111 kB)\n",
      "Downloading plotly-5.19.0-py3-none-any.whl (15.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
      "Downloading jaxtyping-0.3.2-py3-none-any.whl (55 kB)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wadler_lindig-0.1.6-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: wadler-lindig, plotly, fancy-einsum, einops, jaxtyping, tokenizers, transformers, vit_prisma\n",
      "Successfully installed einops-0.8.1 fancy-einsum-0.0.3 jaxtyping-0.3.2 plotly-5.19.0 tokenizers-0.21.1 transformers-4.51.3 vit_prisma-0.1.4 wadler-lindig-0.1.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1131/2562101075.py:89: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"# Load model and data\\n\\n## ViT Architecture\\n\\n![image](https://production-media.paperswithcode.com/methods/Screen_Shot_2021-01-26_at_9.43.31_PM_uI4jjMq.png)\\n\\n\\nA [vision transformer](https://arxiv.org/pdf/2010.11929.pdf) (ViT) is an architecture designed for image classification tasks, similar to the classic transformer architecture used in language models. A ViT consists of transformer blocks; each block consists of an Attention layer and an MLP layer.\\n\\n\\nUnlike language models, vision transformers do not have a dictionary embedding and unembedding matrix. Instead, images are divided into non-overlapping patches, similar to tokens in language models. These patches are flattened and linearly projected to embeddings via a Conv2D layer, similar to word embeddings in language models. A learnable class token (CLS token) is appended to the beginning of the sequence, which accrues global information throughout the network. A linear position embedding is added to the patches.\\n\\nThe patch embeddings then pass through the transformer blocks (each block consists of a layer norm, an attention layer, another layernorm, and an mlp layer). The output of each block is added back to the previous input. The sum of the block and the previous input is called the residual stream.\\n\\nThe final layer of this vision transformer is a classification head with 1000 logit values for ImageNet's 1000 classes. The CLS token is fed into the final layer for 1000-way classification.\\n\\nLike TransformerLens, we use HookedViT to easily capture intermediate activations with custom hook functions, instead of dealing with PyTorch's normal hook functionality.\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"ViT Prisma Main Demo\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1TL_BY1huQ4-OTORKbiIg7XfTyUbmyToQ\n",
    "\n",
    "by Sonia Joseph\n",
    "\n",
    "Twitter: [@soniajoseph_](https://twitter.com/soniajoseph_)\n",
    "\n",
    "Original introduction is [here](https://www.lesswrong.com/posts/kobJymvvcvhbjWFKe/laying-the-foundations-for-vision-and-multimodal-mechanistic).\n",
    "\n",
    "# Introduction\n",
    "\n",
    "The purpose of this notebook is to introduce readers to vision transformer (ViT) mechanistic interpretability.\n",
    "\n",
    "To make ViT mechanistic interpretability easier, I built an [open source library Prisma](https://github.com/soniajoseph/ViT-Prisma). The library is based on Neel Nanda's fantastic [TransformerLens](https://github.com/neelnanda-io/TransformerLens) but adapted for vision transformers and text-image models like CLIP. This notebook serves as a demo of the library. I highly encourage readers to check out the library and request features that they'd like to see!\n",
    "\n",
    "I hope this notebook builds the vision mech interp ecosystem and encourages researchers to pursue their own directions with Prisma!\n",
    "\n",
    "## Audience\n",
    "This notebook is geared toward two audiences. The first is familiar with language mech interp, but not vision mech interp. The second is somewhat new to mechanistic interpretability, but is familiar with basic deep learning and has some mild exposure to mech interp concepts. If you are *completely* new to all mech interp, I recommend getting the basics down with the [ARENA curriculum](https://github.com/callummcdougall/ARENA_2.0) first.\n",
    "\n",
    "\n",
    "The structure of this notebook is based on the excellent notebook [Exploratory Analysis](https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/demos/Exploratory_Analysis_Demo.ipynb#scrollTo=lZgu7cH72kdd) from TransformerLens, with some detours. While this notebook acts as a stand-alone, I encourage readers to consult the original notebook when they would like a deeper explanation.\n",
    "\n",
    "For unfamiliar terms, also check out the [mech interp explorer](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#inline-images&theme=default).\n",
    "\n",
    "## Structure\n",
    "\n",
    "*See the sidebar for navigation.*\n",
    "\n",
    "We'll run through the basic mech interp techniques on a vision transformer, including:\n",
    "\n",
    "* Logit attribution\n",
    "* Patch-level emoji logit lens\n",
    "* Attention visualization\n",
    "* Activation patching\n",
    "\n",
    "**We will illustrate the last few technique by changing the ViT's prediction from tabby cat to Border Collie with a minimum viable ablation.**\n",
    "\n",
    "## Acknowledgements and contributors\n",
    "\n",
    "Thank you to Noah MacCallum, Rob Graham, and Karolis Ramanauskas for giving feedback on an early draft of this notebook.\n",
    "\n",
    "Further thank you to Neel Nanda for your feedback, and to the Prisma team and core contributers, the MATS community, and South Park Commons. Full acknowledgements are on the Prisma repo documentation.\n",
    "\n",
    "### Differences between ViT and Language Interpretability\n",
    "\n",
    "*This section is geared toward readers already familiar with language transformer mech interp. If you are new to mech interp in general, you don't have to dwell on this section too much.*\n",
    "\n",
    "\n",
    "Vision mech interp is like language mech interp, but in a fun-house mirror. Both architectures are transformers, so many LLM techniques carry over. However, there are a few twists:\n",
    "\n",
    "* **The typical ViT is not doing unidirectional sequence modeling.** ViTs use bidirectional attention and predict a global CLS token, rather than predicting the next token in an autoregressive manner. (Note: There are autoregressive vision transformers with basically the same architecture as language, such as [Image GPT](https://openai.com/research/image-gpt) and [Parti](https://sites.research.google/parti/), which do next-token image generation. However, as of February 2024, autoregressive vision transformers are not frequently used in the wild.)\n",
    "* **Bidirectional attention vs causal attention.** Language transformers have causal (unidirectional) attention. This means that there is an upper triangular mask on the attention, so that earlier tokens cannot attend to tokens in the future. The classical ViT, with its bidirectional attention, does not have the same concept of \"time.\" Thus, some of the original LLM mech interp techniques break. It can be unclear which direction information is flowing. Induction heads, if they are present in vision, would look different from those in language to account for bidirectional attention.\n",
    "* **CLS token instead of next token prediction/ autoregressive loss.** For ViTs, a learnable CLS token, which is prepended to the input, gets fed into the classification head instead of the final token as in language. The CLS token accrues global information from the other patches through self-attention as all the patches pass through the net.\n",
    "* **No canonical dictionary matrix. Vision is more ambiguous.** Vision lacks a standard dictionary matrix like the 50k one for language, partially due to inherent ambiguity. For instance, a yellow patch on a goldfinch might represent \"yellow,\" \"wing,\" \"goldfinch,\" \"bird,\" or \"animal,\" depending on the granularity, demonstrating hierarchical ambiguity. An animal might be identified specifically as a \"Border collie\" or more generally as a \"dog.\" Beyond hierarchy, ambiguity in vision also stems from cultural interpretations and the imprecision of language. Practically, ImageNet's 1000 classes serve as a makeshift \"dictionary,\" but it falls short of fully encompassing visual concepts.\n",
    "* **Additional hyperparameters.** Patch size is a vision-specific hyperparameter, determining the size of the patches into which an image is divided. Using smaller patches increases accuracy but also computational load, because attention scales quadratically with patch number.\n",
    "* **There is a zoo of vision transformers.** Similar to language, vision transformers come in many forms. The most relevant are the vanilla ViT, which we'll be analyzing in this notebook; CLIP, which is co-trained with text using contrastive loss; and DINO uses unlabeled data. For a review, check out [this survey](https://arxiv.org/pdf/2101.01169.pdf).\n",
    "\n",
    "## Import libraries, data, and helper functions (ignore)\n",
    "\"\"\"\n",
    "\n",
    "# Install the Prisma repo library (update version number or clone from source for latest functionality)\n",
    "\n",
    "!pip install vit_prisma\n",
    "\n",
    "import vit_prisma\n",
    "from vit_prisma.utils.data_utils.imagenet_dict import IMAGENET_DICT\n",
    "from vit_prisma.utils import prisma_utils\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from fancy_einsum import einsum\n",
    "from collections import defaultdict\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# Get images we'll feed into the model\n",
    "!wget https://github.com/soniajoseph/ViT-Prisma/blob/main/src/vit_prisma/sample_images/cat_dog.jpeg?raw=true -O cat_dog.jpeg --quiet\n",
    "!wget https://github.com/soniajoseph/ViT-Prisma/blob/main/src/vit_prisma/sample_images/cat_crop.jpeg?raw=true -O crop_cat.png --quiet\n",
    "\n",
    "\"\"\"**Helper Functions** (ignore)\"\"\"\n",
    "\n",
    "# Helper function (ignore)\n",
    "def plot_image(image):\n",
    "  plt.figure()\n",
    "  plt.axis('off')\n",
    "  plt.imshow(image.permute(1,2,0))\n",
    "\n",
    "class ConvertTo3Channels:\n",
    "    def __call__(self, img):\n",
    "        if img.mode != 'RGB':\n",
    "            return img.convert('RGB')\n",
    "        return img\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    ConvertTo3Channels(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def plot_logit_boxplot(average_logits, labels):\n",
    "  hovertexts = np.array([[IMAGENET_DICT[i] for _ in range(25)] for i in range(1000)])\n",
    "\n",
    "  fig = go.Figure()\n",
    "  data = []\n",
    "\n",
    "  # if tensor, turn to numpy\n",
    "  if isinstance(average_logits, torch.Tensor):\n",
    "      average_logits = average_logits.detach().cpu().numpy()\n",
    "\n",
    "  for i in range(average_logits.shape[1]):  # For each layer\n",
    "      layer_logits = average_logits[:, i]\n",
    "      hovertext = hovertexts[:, i]\n",
    "      box = fig.add_trace(go.Box(\n",
    "          y=layer_logits,\n",
    "          name=f'{layer_labels[i]}',\n",
    "          text=hovertext,\n",
    "          hoverinfo='y+text',\n",
    "          boxpoints='suspectedoutliers'\n",
    "      ))\n",
    "      data.append(box)\n",
    "\n",
    "\n",
    "  means = np.mean(average_logits, axis=0)\n",
    "  fig.add_trace(go.Scatter(\n",
    "      x = layer_labels,\n",
    "      y=means,\n",
    "      mode='markers',\n",
    "      name='Mean',\n",
    "      # line=dict(color='gray'),\n",
    "      marker=dict(size=4, color='red'),\n",
    "  ))\n",
    "\n",
    "\n",
    "  fig.update_layout(\n",
    "      title='Raw Logit Values Per Layer (each dot is 1 ImageNet Class)',\n",
    "      xaxis=dict(title='Layer'),\n",
    "      yaxis=dict(title='Logit Values'),\n",
    "      showlegend=False\n",
    "  )\n",
    "\n",
    "  fig.show()\n",
    "\n",
    "def plot_patched_component(patched_head, title=''):\n",
    "  \"\"\"\n",
    "  Use for plotting Activation Patching.\n",
    "  \"\"\"\n",
    "\n",
    "  fig = go.Figure(data=go.Heatmap(\n",
    "      z=patched_head.detach().numpy(),\n",
    "      colorscale='RdBu',  # You can choose any colorscale\n",
    "      colorbar=dict(title='Value'),  # Customize the color bar\n",
    "      hoverongaps=False\n",
    "  ))\n",
    "  fig.update_layout(\n",
    "      title=title,\n",
    "      xaxis_title='Attention Head',\n",
    "      yaxis_title='Patch Number',\n",
    "  )\n",
    "\n",
    "  return fig\n",
    "\n",
    "def imshow(tensor, **kwargs):\n",
    "    \"\"\"\n",
    "    Use for Activation Patching.\n",
    "    \"\"\"\n",
    "    px.imshow(\n",
    "          prisma_utils.to_numpy(tensor),\n",
    "          color_continuous_midpoint=0.0,\n",
    "          color_continuous_scale=\"RdBu\",\n",
    "          **kwargs,\n",
    "      ).show()\n",
    "\n",
    "\"\"\"# Load model and data\n",
    "\n",
    "## ViT Architecture\n",
    "\n",
    "![image](https://production-media.paperswithcode.com/methods/Screen_Shot_2021-01-26_at_9.43.31_PM_uI4jjMq.png)\n",
    "\n",
    "\n",
    "A [vision transformer](https://arxiv.org/pdf/2010.11929.pdf) (ViT) is an architecture designed for image classification tasks, similar to the classic transformer architecture used in language models. A ViT consists of transformer blocks; each block consists of an Attention layer and an MLP layer.\n",
    "\n",
    "\n",
    "Unlike language models, vision transformers do not have a dictionary embedding and unembedding matrix. Instead, images are divided into non-overlapping patches, similar to tokens in language models. These patches are flattened and linearly projected to embeddings via a Conv2D layer, similar to word embeddings in language models. A learnable class token (CLS token) is appended to the beginning of the sequence, which accrues global information throughout the network. A linear position embedding is added to the patches.\n",
    "\n",
    "The patch embeddings then pass through the transformer blocks (each block consists of a layer norm, an attention layer, another layernorm, and an mlp layer). The output of each block is added back to the previous input. The sum of the block and the previous input is called the residual stream.\n",
    "\n",
    "The final layer of this vision transformer is a classification head with 1000 logit values for ImageNet's 1000 classes. The CLS token is fed into the final layer for 1000-way classification.\n",
    "\n",
    "Like TransformerLens, we use HookedViT to easily capture intermediate activations with custom hook functions, instead of dealing with PyTorch's normal hook functionality.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3818c4b3-8fe1-40a3-b143-a72a03e7fe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_prisma.models.base_vit import HookedViT\n",
    "from vit_prisma.configs.HookedViTConfig import HookedViTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4402df29-f9e3-4867-8411-ec53155133bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53782a7ca7d14652acd404bc78b6bd1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/586 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'TimmWrapperConfig' object has no attribute 'num_hidden_layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvit_prisma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_vit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HookedViT\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvit_prisma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfigs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mHookedViTConfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HookedViTConfig\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m HookedViT\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvit_base_patch16_384\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m                                         center_writing_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m                                         center_unembed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m                                         fold_ln\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m                                         refactor_factored_attn_matrices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m                                           \n\u001b[1;32m     10\u001b[0m                                     )\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/vit_prisma/models/base_vit.py:723\u001b[0m, in \u001b[0;36mHookedViT.from_pretrained\u001b[0;34m(cls, model_name, is_timm, is_clip, fold_ln, center_writing_weights, refactor_factored_attn_matrices, checkpoint_index, checkpoint_value, hf_model, device, n_devices, move_to_device, fold_value_biases, default_prepend_bos, default_padding_side, dtype, use_attn_result, **from_pretrained_kwargs)\u001b[0m\n\u001b[1;32m    717\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    718\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat16 models may not work on CPU. Consider using a GPU or bfloat16.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    719\u001b[0m     )\n\u001b[1;32m    721\u001b[0m \u001b[38;5;66;03m# Set up other parts of transformer\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m cfg \u001b[38;5;241m=\u001b[39m convert_pretrained_model_config(\n\u001b[1;32m    724\u001b[0m     model_name,\n\u001b[1;32m    725\u001b[0m     is_timm\u001b[38;5;241m=\u001b[39mis_timm,\n\u001b[1;32m    726\u001b[0m     is_clip\u001b[38;5;241m=\u001b[39mis_clip,\n\u001b[1;32m    727\u001b[0m )\n\u001b[1;32m    731\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m get_pretrained_state_dict(\n\u001b[1;32m    732\u001b[0m     model_name, is_timm, is_clip, cfg, hf_model, dtype\u001b[38;5;241m=\u001b[39mdtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfrom_pretrained_kwargs\n\u001b[1;32m    733\u001b[0m )\n\u001b[1;32m    735\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(cfg, move_to_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/vit_prisma/prisma_tools/loading_from_pretrained.py:354\u001b[0m, in \u001b[0;36mconvert_pretrained_model_config\u001b[0;34m(model_name, is_timm, is_clip)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(hf_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtubelet_size\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    351\u001b[0m     ps \u001b[38;5;241m=\u001b[39m hf_config\u001b[38;5;241m.\u001b[39mtubelet_size[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    353\u001b[0m pretrained_config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 354\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_layers\u001b[39m\u001b[38;5;124m'\u001b[39m : hf_config\u001b[38;5;241m.\u001b[39mnum_hidden_layers,\n\u001b[1;32m    355\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md_model\u001b[39m\u001b[38;5;124m'\u001b[39m : hf_config\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    356\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md_head\u001b[39m\u001b[38;5;124m'\u001b[39m : hf_config\u001b[38;5;241m.\u001b[39mhidden_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m hf_config\u001b[38;5;241m.\u001b[39mnum_attention_heads,\n\u001b[1;32m    357\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m : hf_config\u001b[38;5;241m.\u001b[39m_name_or_path,\n\u001b[1;32m    358\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_heads\u001b[39m\u001b[38;5;124m'\u001b[39m : hf_config\u001b[38;5;241m.\u001b[39mnum_attention_heads,\n\u001b[1;32m    359\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md_mlp\u001b[39m\u001b[38;5;124m'\u001b[39m : hf_config\u001b[38;5;241m.\u001b[39mintermediate_size,\n\u001b[1;32m    360\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation_name\u001b[39m\u001b[38;5;124m'\u001b[39m : hf_config\u001b[38;5;241m.\u001b[39mhidden_act,\n\u001b[1;32m    361\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m : hf_config\u001b[38;5;241m.\u001b[39mlayer_norm_eps,\n\u001b[1;32m    362\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal_architecture\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28mgetattr\u001b[39m(hf_config, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marchitecture\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mgetattr\u001b[39m(hf_config, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marchitectures\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[1;32m    363\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitializer_range\u001b[39m\u001b[38;5;124m'\u001b[39m : hf_config\u001b[38;5;241m.\u001b[39minitializer_range,\n\u001b[1;32m    364\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_channels\u001b[39m\u001b[38;5;124m'\u001b[39m : hf_config\u001b[38;5;241m.\u001b[39mnum_channels,\n\u001b[1;32m    365\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m : ps,\n\u001b[1;32m    366\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_size\u001b[39m\u001b[38;5;124m'\u001b[39m : hf_config\u001b[38;5;241m.\u001b[39mimage_size,\n\u001b[1;32m    367\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_classes\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28mgetattr\u001b[39m(hf_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    368\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_params\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters() \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad) \u001b[38;5;28;01mif\u001b[39;00m is_timm \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    369\u001b[0m             }\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# Rectifying Huggingface bugs:\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# Currently a bug getting configs, only this model confirmed to work and even it requires modification of eps\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_timm \u001b[38;5;129;01mand\u001b[39;00m model_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvit_base_patch16_224\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/configuration_utils.py:210\u001b[0m, in \u001b[0;36mPretrainedConfig.__getattribute__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    209\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m)[key]\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TimmWrapperConfig' object has no attribute 'num_hidden_layers'"
     ]
    }
   ],
   "source": [
    "from vit_prisma.models.base_vit import HookedViT\n",
    "from vit_prisma.configs.HookedViTConfig import HookedViTConfig\n",
    "\n",
    "model = HookedViT.from_pretrained(\"vit_base_patch16_384\",\n",
    "                                        center_writing_weights=True,\n",
    "                                        center_unembed=True,\n",
    "                                        fold_ln=True,\n",
    "                                        refactor_factored_attn_matrices=True,\n",
    "                                          \n",
    "                                    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
